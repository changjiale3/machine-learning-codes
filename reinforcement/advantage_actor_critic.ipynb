{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "advantage_actor_critic.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0-n1yvnT7TK",
        "colab_type": "text"
      },
      "source": [
        "# Advantage Actor-Critic\n",
        "\n",
        "## Loss\n",
        "### Policy Network\n",
        "$$\\bigtriangledown_{\\theta}\\text{J}(\\theta) = \\frac{1}{N}\\sum_{i=1}^N{ [ \\sum_{t=0}^T{ [\\bigtriangledown_{\\theta} \\text{log} \\pi_{\\theta}(a_{i, t}|s_{i, t}) (r(s_{i, t}, a_{i, t}) + \\gamma v(s_{i, t+1}) - v(i, s_t))] }] }$$\n",
        "\n",
        "If the episode length is $\\infty$, then $v(s)$ can get infinitely large in many cases.\n",
        "\n",
        "Simple tricks: better to get rewards sooner than later + discount factor\n",
        "\n",
        "$$y_{i, t} = r(s_{i, t}, a_{i, t}) + \\gamma v(s_{i, t+1})$$\n",
        "\n",
        "where $\\gamma \\in [0, 1]$(0.99 works well)\n",
        "\n",
        "Note that we use $\\gamma v(s_{i, t+1})$ instead of $\\gamma q(s_{i, t+1},  a_{i, t+1})$ because we choose the maximum action. In other words, the Value network is essentially the maximum future reward.\n",
        "\n",
        "### Value Network\n",
        "$$\\bigtriangledown_{\\theta}\\text{J}(\\theta) = \\frac{1}{N}\\sum_{i=1}^N{ [ \\sum_{t=0}^T{[ \\text{smooth_l1_loss}(v(s_{i, t}), r(s_{i, t}, a_{i, t}) + \\gamma v(s_{i, t+1}) )  ]}] }$$\n",
        " \n",
        " \n",
        "## Coding Notes\n",
        "+ In this code, the reward should be divided by 100\n",
        "\n",
        "+ ReLU is very important\n",
        " \n",
        "+ $\\gamma v(s_{i, t+1})$ can be multiplied by (1-done). In other words, if this (s, a) pair terminates this episode, then $\\gamma v(s_{i, t+1})$ is not required.\n",
        "\n",
        "+ Update the parameters during the episode. However, the Policy Gradient updates the parameters after one episode\n",
        "\n",
        "## Questions\n",
        "\n",
        "+ Why does the reward should be multiplied by 100?\n",
        "\n",
        "+ What role does the rollout perform?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYB1UwHdWuaM",
        "colab_type": "text"
      },
      "source": [
        "## 1. Import packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vz60_-mZT4eg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.distributions import Categorical\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sP3o5Ap_WwIv",
        "colab_type": "text"
      },
      "source": [
        "## 2. Define constants"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yvCpKLYMWt_4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gamma = 0.98\n",
        "num_epochs = 3000\n",
        "num_rollouts = 5\n",
        "reward_div = 100\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RrRjDJBlWxpq",
        "colab_type": "text"
      },
      "source": [
        "## 3. Prepare data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qr55DooZW3hE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env = gym.make(\"CartPole-v0\")\n",
        "\n",
        "def get_sample(env, policy):\n",
        "    done = False\n",
        "    s = env.reset() # (state_size, )\n",
        "    while not done:\n",
        "        ss, aa, rr, nss, dones, log_probs = list(), list(), list(), list(), list(), list()\n",
        "        for t in range(num_rollouts):\n",
        "            a, log_prob = policy.predict(torch.Tensor(s).to(device))\n",
        "            ns, r, done, _ = env.step(a) # a is 0 or 1\n",
        "            ss.append(s)\n",
        "            aa.append(a)\n",
        "            rr.append(r)\n",
        "            nss.append(ns)\n",
        "            dones.append(done)\n",
        "            log_probs.append(log_prob) # log probability of the current action\n",
        "            s = ns\n",
        "            if done:\n",
        "                break\n",
        "                \n",
        "        sample = (torch.Tensor(ss).to(device), torch.LongTensor(aa).to(device), torch.Tensor(rr).to(device), torch.Tensor(nss).to(device), torch.Tensor(dones).to(device))\n",
        "        log_probs = torch.Tensor(log_probs).to(device)\n",
        "        yield sample, log_probs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xn5nllGMWzaG",
        "colab_type": "text"
      },
      "source": [
        "## 4. Build model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLJNaXeUXD_l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ActorCritic(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ActorCritic, self).__init__()\n",
        "        \n",
        "        self.fc1 = nn.Linear(4, 256)\n",
        "        self.fc_pi = nn.Linear(256, 2)\n",
        "        self.fc_v = nn.Linear(256, 1)\n",
        "        \n",
        "        self.optimizer = torch.optim.Adam(self.parameters(), lr=0.0002, betas=(0.9, 0.99))\n",
        "\n",
        "    def policy(self, state, softmax_dim=0):\n",
        "        net = F.relu(self.fc1(state)) # (B, 256) # !!! Do not forget ReLU\n",
        "        net = self.fc_pi(net) # (B, 2)\n",
        "        probs = F.softmax(net, dim=softmax_dim)\n",
        "        return probs\n",
        "        \n",
        "    def predict(self, state, softmax_dim=0): # state: (4,) => indicates that the fully-connected layer in PyTorch can receive inputs without batch_size\n",
        "        probs = self.policy(state)\n",
        "        m = Categorical(probs) # !!! The cpu or gpu version will influence the seed. In other words, even if we set the seed to be 2, different versions of `probs` might produce different results\n",
        "        a_pred = m.sample().item()\n",
        "        return a_pred, torch.log(probs[a_pred]) # (predicted action: 0 or 1, log of probability of current action)\n",
        "\n",
        "    def value(self, state):\n",
        "        net = F.relu(self.fc1(state)) # !!! Do not forget ReLU\n",
        "        return self.fc_v(net)\n",
        "      \n",
        "    def fit(self, sample, log_probs): # samples: [(s1, a1, r1), (s2, a2, r2), ...], log_probs: (log_prob1, log_prob2, ...)\n",
        "        (s, a, r, ns, done) = sample\n",
        "        \n",
        "        r /= reward_div # !!! divide by 100 is very important\n",
        "        td_target = (r + gamma * self.value(ns).squeeze() * (1 - done)).unsqueeze(1) # (num_rollouts, 1)\n",
        "        vs = self.value(s) # (num_rollouts, 1)\n",
        "        delta = td_target - vs # (num_rollouts, 1)\n",
        "        \n",
        "        probs = self.policy(s, softmax_dim=1) # (num_rollouts, action_size=2)\n",
        "        probs = probs.gather(1, a.unsqueeze(1)) # (num_rollouts, 1)\n",
        "        loss = torch.mean(-torch.log(probs) * delta.detach() +  F.smooth_l1_loss(td_target.detach(), vs))\n",
        "        \n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        \n",
        "ac = ActorCritic().to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjsEItDbW1SM",
        "colab_type": "text"
      },
      "source": [
        "## 5. Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUzvlHRuXFRi",
        "colab_type": "code",
        "outputId": "d5e0aeb1-6fc8-40e0-97f3-1c87ed0528fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        }
      },
      "source": [
        "score = 0.0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    sample_iter = get_sample(env, ac)\n",
        "    for sample, log_probs in sample_iter:\n",
        "        ac.fit(sample, log_probs)\n",
        "        rewards = sample[2] * reward_div\n",
        "        score += sum(rewards)\n",
        "        \n",
        "    if epoch % 100 == 0:\n",
        "        print('Epoch %d || Average Score: %.6f'%(epoch, score / (epoch + 1)))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 || Average Score: 20.000000\n",
            "Epoch 100 || Average Score: 26.128712\n",
            "Epoch 200 || Average Score: 35.134327\n",
            "Epoch 300 || Average Score: 52.196011\n",
            "Epoch 400 || Average Score: 79.152122\n",
            "Epoch 500 || Average Score: 100.013969\n",
            "Epoch 600 || Average Score: 115.379364\n",
            "Epoch 700 || Average Score: 124.440804\n",
            "Epoch 800 || Average Score: 131.541824\n",
            "Epoch 900 || Average Score: 136.172043\n",
            "Epoch 1000 || Average Score: 140.979019\n",
            "Epoch 1100 || Average Score: 143.283386\n",
            "Epoch 1200 || Average Score: 146.080765\n",
            "Epoch 1300 || Average Score: 147.308228\n",
            "Epoch 1400 || Average Score: 149.441818\n",
            "Epoch 1500 || Average Score: 152.145233\n",
            "Epoch 1600 || Average Score: 154.976898\n",
            "Epoch 1700 || Average Score: 153.510864\n",
            "Epoch 1900 || Average Score: 149.038406\n",
            "Epoch 2000 || Average Score: 147.919540\n",
            "Epoch 2100 || Average Score: 146.827698\n",
            "Epoch 2200 || Average Score: 143.709229\n",
            "Epoch 2300 || Average Score: 141.063019\n",
            "Epoch 2400 || Average Score: 138.956268\n",
            "Epoch 2500 || Average Score: 135.730103\n",
            "Epoch 2600 || Average Score: 133.369095\n",
            "Epoch 2800 || Average Score: 129.325241\n",
            "Epoch 2900 || Average Score: 127.670456\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}