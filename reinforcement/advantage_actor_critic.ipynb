{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "advantage_actor_critic.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0-n1yvnT7TK",
        "colab_type": "text"
      },
      "source": [
        "# Advantage Actor-Critic\n",
        "\n",
        "## Loss\n",
        "### 1. Policy Network\n",
        "$$\\bigtriangledown_{\\theta}\\text{J}(\\theta) = \\frac{1}{N}\\sum_{i=1}^N{ [ \\sum_{t=0}^T{ [\\bigtriangledown_{\\theta} \\text{log} \\pi_{\\theta}(a_{i, t}|s_{i, t}) (r(s_{i, t}, a_{i, t}) + \\gamma v(s_{i, t+1}) - v(i, s_t))] }] }$$\n",
        "\n",
        "If the episode length is $\\infty$, then $v(s)$ can get infinitely large in many cases.\n",
        "\n",
        "Simple tricks: better to get rewards sooner than later + discount factor\n",
        "\n",
        "$$y_{i, t} = r(s_{i, t}, a_{i, t}) + \\gamma v(s_{i, t+1})$$\n",
        "\n",
        "where $\\gamma \\in [0, 1]$(0.99 works well)\n",
        "\n",
        "Note that we use $\\gamma v(s_{i, t+1})$ instead of $\\gamma q(s_{i, t+1},  a_{i, t+1})$ because we choose the maximum action. In other words, the Value network is essentially the maximum future reward.\n",
        "\n",
        "### 2. Value Network\n",
        "$$\\bigtriangledown_{\\theta}\\text{J}(\\theta) = \\frac{1}{N}\\sum_{i=1}^N{ [ \\sum_{t=0}^T{[ \\text{smooth_l1_loss}(v(s_{i, t}), r(s_{i, t}, a_{i, t}) + \\gamma v(s_{i, t+1}) )  ]}] }$$\n",
        " \n",
        " \n",
        "## Points\n",
        "+ In this code, the reward should be divided by 100\n",
        "\n",
        "+ ReLU is very important\n",
        " \n",
        "+ $\\gamma v(s_{i, t+1})$ can be multiplied by (1-done). In other words, if this (s, a) pair terminates this episode, then $\\gamma v(s_{i, t+1})$ is not required.\n",
        "\n",
        "+ Update the parameters during the episode. However, the Policy Gradient updates the parameters after one episode\n",
        "\n",
        "+ Regression problem, so both MSE loss and Smooth L1 Loss ( See details [here](https://stats.stackexchange.com/questions/351874/how-to-interpret-smooth-l1-loss)) work well: $$L_{1;smooth} = \\begin{cases}|x| & \\text{if $|x|>\\alpha$;} \\\\\n",
        "\\frac{1}{|\\alpha|}x^2 & \\text{if $|x| \\leq \\alpha$}\\end{cases}$$\n",
        "\n",
        "## Questions\n",
        "\n",
        "+ Why does the reward should be multiplied by 100?\n",
        "\n",
        "+ What role does the rollout perform?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYB1UwHdWuaM",
        "colab_type": "text"
      },
      "source": [
        "## 1. Import packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vz60_-mZT4eg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.distributions import Categorical\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sP3o5Ap_WwIv",
        "colab_type": "text"
      },
      "source": [
        "## 2. Define constants"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yvCpKLYMWt_4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gamma = 0.98\n",
        "num_epochs = 3000\n",
        "num_rollouts = 5\n",
        "reward_div = 100\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RrRjDJBlWxpq",
        "colab_type": "text"
      },
      "source": [
        "## 3. Prepare data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qr55DooZW3hE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env = gym.make(\"CartPole-v0\")\n",
        "\n",
        "def get_sample(env, policy):\n",
        "    done = False\n",
        "    s = env.reset() # (state_size, )\n",
        "    while not done:\n",
        "        ss, aa, rr, s_primes, done_masks = list(), list(), list(), list(), list()\n",
        "        for t in range(num_rollouts):\n",
        "            a = policy.sample_action(s)\n",
        "            s_prime, r, done, _ = env.step(a) # a is 0 or 1\n",
        "            ss.append(s)\n",
        "            aa.append(a)\n",
        "            rr.append(r)\n",
        "            s_primes.append(s_prime)\n",
        "            done_mask = 0.0 if done else 1.0\n",
        "            done_masks.append(done_mask)\n",
        "            s = s_prime\n",
        "            if done:\n",
        "                break\n",
        "                \n",
        "        sample = (torch.Tensor(ss).to(device), torch.LongTensor(aa).to(device), torch.Tensor(rr).to(device), torch.Tensor(s_primes).to(device), torch.Tensor(done_masks).to(device))\n",
        "        yield sample"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xn5nllGMWzaG",
        "colab_type": "text"
      },
      "source": [
        "## 4. Build model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLJNaXeUXD_l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ActorCritic(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ActorCritic, self).__init__()\n",
        "        \n",
        "        self.fc1 = nn.Linear(4, 256)\n",
        "        self.fc_pi = nn.Linear(256, 2)\n",
        "        self.fc_v = nn.Linear(256, 1)\n",
        "        \n",
        "        self.optimizer = torch.optim.Adam(self.parameters(), lr=0.0002, betas=(0.9, 0.99))\n",
        "\n",
        "    def policy(self, state, softmax_dim=0):\n",
        "        net = F.relu(self.fc1(state)) # (B, 256) # !!! Do not forget ReLU\n",
        "        net = self.fc_pi(net) # (B, 2)\n",
        "        probs = F.softmax(net, dim=softmax_dim)\n",
        "        return probs\n",
        "        \n",
        "    def sample_action(self, state, softmax_dim=0): # state: (4,) => indicates that the fully-connected layer in PyTorch can receive inputs without batch_size\n",
        "        state = torch.Tensor(state).to(device)\n",
        "        probs = self.policy(state)\n",
        "        m = Categorical(probs) # !!! The cpu or gpu version will influence the seed. In other words, even if we set the seed to be 2, different versions of `probs` might produce different results\n",
        "        a_pred = m.sample().item()\n",
        "        return a_pred # (predicted action: 0 or 1, log of probability of current action)\n",
        "\n",
        "    def value(self, state):\n",
        "        net = F.relu(self.fc1(state)) # !!! Do not forget ReLU\n",
        "        return self.fc_v(net)\n",
        "      \n",
        "    def fit(self, sample): # samples: [(s1, a1, r1), (s2, a2, r2), ...]\n",
        "        (s, a, r, ns, done_mask) = sample\n",
        "        \n",
        "        r /= reward_div # !!! divide by 100 is very important\n",
        "        td_target = (r + gamma * self.value(ns).squeeze() * done_mask).unsqueeze(1) # (num_rollouts, 1)\n",
        "        vs = self.value(s) # (num_rollouts, 1)\n",
        "        delta = td_target - vs # (num_rollouts, 1)\n",
        "        \n",
        "        probs = self.policy(s, softmax_dim=1) # (num_rollouts, action_size=2)\n",
        "        probs = probs.gather(1, a.unsqueeze(1)) # (num_rollouts, 1)\n",
        "        loss = torch.mean(-torch.log(probs) * delta.detach() +  F.smooth_l1_loss(vs, td_target.detach()))\n",
        "#         loss = torch.mean(-torch.log(probs) * delta.detach() +  F.mse_loss(vs, td_target.detach())) # Work well, too\n",
        "        \n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        \n",
        "ac = ActorCritic().to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjsEItDbW1SM",
        "colab_type": "text"
      },
      "source": [
        "## 5. Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxAb0gBKhI_f",
        "colab_type": "code",
        "outputId": "0bf8b589-15d1-46ae-b434-52700afb2b44",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        }
      },
      "source": [
        "# mse(vs, td_target)\n",
        "score = 0.0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    sample_iter = get_sample(env, ac)\n",
        "    for sample in sample_iter:\n",
        "        ac.fit(sample)\n",
        "        rewards = sample[2] * reward_div\n",
        "        score += sum(rewards)\n",
        "        \n",
        "    if epoch % 100 == 0:\n",
        "        print('Epoch %d || Average Score: %.6f'%(epoch, score / (epoch + 1)))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 || Average Score: 13.000000\n",
            "Epoch 100 || Average Score: 25.623762\n",
            "Epoch 200 || Average Score: 36.805969\n",
            "Epoch 300 || Average Score: 64.820595\n",
            "Epoch 400 || Average Score: 81.882797\n",
            "Epoch 500 || Average Score: 96.031937\n",
            "Epoch 600 || Average Score: 108.652245\n",
            "Epoch 700 || Average Score: 118.460777\n",
            "Epoch 800 || Average Score: 126.640450\n",
            "Epoch 900 || Average Score: 134.659271\n",
            "Epoch 1000 || Average Score: 139.136856\n",
            "Epoch 1100 || Average Score: 142.532242\n",
            "Epoch 1200 || Average Score: 146.371353\n",
            "Epoch 1300 || Average Score: 149.510376\n",
            "Epoch 1400 || Average Score: 153.055679\n",
            "Epoch 1500 || Average Score: 154.393738\n",
            "Epoch 1600 || Average Score: 152.937546\n",
            "Epoch 1700 || Average Score: 155.192230\n",
            "Epoch 1800 || Average Score: 154.067184\n",
            "Epoch 1900 || Average Score: 155.566544\n",
            "Epoch 2000 || Average Score: 157.206894\n",
            "Epoch 2100 || Average Score: 156.545456\n",
            "Epoch 2200 || Average Score: 155.346207\n",
            "Epoch 2300 || Average Score: 154.759674\n",
            "Epoch 2400 || Average Score: 154.353180\n",
            "Epoch 2500 || Average Score: 153.932434\n",
            "Epoch 2600 || Average Score: 154.442520\n",
            "Epoch 2700 || Average Score: 155.984085\n",
            "Epoch 2800 || Average Score: 155.406647\n",
            "Epoch 2900 || Average Score: 155.346420\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}