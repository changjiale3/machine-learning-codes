{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "proximal_policy_optimization_lstm.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHMyZNNQ1GMW",
        "colab_type": "text"
      },
      "source": [
        "# Proximal Policy Optimization with LSTM\n",
        "\n",
        "[Reference code](https://github.com/seungeunrho/minimalRL/blob/master/ppo-lstm.py)\n",
        "\n",
        "## Points\n",
        "+ Model predicts values and probabilities based on hidden states and environment states\n",
        "\n",
        "+ The time sequence if LSTM corresponds to a complete interation sequence\n",
        "\n",
        "+ Actor Network handles 1 timestep, while Critic Network handles B timesteps, where B means not only the batch size, but also the length of rollouts.\n",
        "\n",
        "+ The initial lstm state in Critic Network is the first output lstm state for 1 rollout. (Not necessarily)\n",
        "\n",
        "## Puzzles\n",
        "+ Why does the result become poor if we use empty lstm_size in the training procedure?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtiS0pgl1dIm",
        "colab_type": "text"
      },
      "source": [
        "## 1. Import packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ya7nPhU08dE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.distributions import Categorical\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVubvfmh1eg3",
        "colab_type": "text"
      },
      "source": [
        "## 2. Define constants"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gHxEggJ01mMP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gamma = 0.98\n",
        "lmbda = 0.95\n",
        "num_epochs = 1000\n",
        "num_rollouts = 20\n",
        "reward_div = 100\n",
        "k_epoch = 3\n",
        "eps = 0.1\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pA8t4-JM1gVl",
        "colab_type": "text"
      },
      "source": [
        "## 3. Prepare data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_BZsj6Ap1oOH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env = gym.make(\"CartPole-v0\")\n",
        "\n",
        "def get_sample(env, policy):\n",
        "    lstm_state = (torch.zeros((1, 1, 32), dtype=torch.float).to(device), torch.zeros((1, 1, 32), dtype=torch.float).to(device))\n",
        "    done = False\n",
        "    s = env.reset() # (state_size, )\n",
        "    while not done:\n",
        "        ss, aa, rr, s_primes, done_masks = list(), list(), list(), list(), list()\n",
        "        probs = list()\n",
        "        first_lstm_state = None\n",
        "        for t in range(num_rollouts):\n",
        "#             if first_lstm_state is None: # Also work!\n",
        "#                 first_lstm_state = lstm_state\n",
        "            a, lstm_state = policy.sample_action(torch.Tensor(s).to(device), lstm_state) # !!! old states should be replaced by new states\n",
        "            s_prime, r, done, _ = env.step(a) # a is 0 or 1\n",
        "            ss.append(s)\n",
        "            aa.append(a)\n",
        "            rr.append(r)\n",
        "            s_primes.append(s_prime)\n",
        "            done_mask = 0.0 if done else 1.0\n",
        "            done_masks.append(done_mask)\n",
        "            probs.append(policy.policy(torch.Tensor(s).to(device), lstm_state)[0][0][a]) # policy.policy output: (probs: (1, 2), lstm_states)\n",
        "            if first_lstm_state is None:\n",
        "                first_lstm_state = lstm_state\n",
        "            s = s_prime\n",
        "            if done:\n",
        "                break\n",
        "                \n",
        "        sample = (torch.Tensor(ss).to(device), torch.LongTensor(aa).to(device), \\\n",
        "                  torch.Tensor(rr).to(device), torch.Tensor(s_primes).to(device), \\\n",
        "                  torch.Tensor(done_masks).to(device), torch.Tensor(probs).to(device), first_lstm_state) # !!! return the first lstm states\n",
        "        yield sample"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AyCIlAsX1huP",
        "colab_type": "text"
      },
      "source": [
        "## 4. Build model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y1FW0de31n54",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def GAE(advantages, gamma, lmbda):\n",
        "    gae_advantages = torch.zeros_like(advantages)\n",
        "    gae = 0\n",
        "\n",
        "    for ri in reversed(range(len(advantages))):\n",
        "        gae = gae * gamma * lmbda + advantages[ri]\n",
        "        gae_advantages[ri] = gae\n",
        "    return gae_advantages\n",
        "\n",
        "\n",
        "class PPO(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(PPO, self).__init__()\n",
        "        \n",
        "        self.fc = nn.Linear(4, 64)\n",
        "        self.lstm = nn.LSTM(64, 32, batch_first=True)\n",
        "        self.fc_pi = nn.Linear(32, 2)\n",
        "        self.fc_v = nn.Linear(32, 1)\n",
        "        \n",
        "        self.optimizer = torch.optim.Adam(self.parameters(), lr=0.0005, betas=(0.9, 0.99))\n",
        "\n",
        "    def policy(self, states, lstm_states, softmax_dim=1):\n",
        "        '''\n",
        "            Input:\n",
        "                states: (B, state_size=4)\n",
        "                lstm_states: (B, 1, hidden_size=32)\n",
        "        '''\n",
        "        net = F.relu(self.fc(states)) # (B, 64)\n",
        "        net = net.view(1, -1, 64) # (1, timestep=B, 64)\n",
        "        net, lstm_states = self.lstm(net, lstm_states) # (1, timestep=B, 32)], ((1, 1, hidden_size=32), (1, 1, hidden_size=32))\n",
        "        net = self.fc_pi(net.squeeze(0)) # (B, 2)\n",
        "        probs = F.softmax(net, dim=softmax_dim)\n",
        "        return probs, lstm_states\n",
        "        \n",
        "    def sample_action(self, state, lstm_states): # state: (4,) => indicates that the fully-connected layer in PyTorch can receive inputs without batch_size\n",
        "        probs, lstm_states = self.policy(state, lstm_states) # (1, 2), lstm_states\n",
        "        prbos = probs.view(2) # (2, )\n",
        "        m = Categorical(probs)\n",
        "        a_pred = m.sample().item()\n",
        "        return a_pred, lstm_states # (predicted action: 0 or 1, log of probability of current action)\n",
        "\n",
        "    def value(self, states, lstm_states):\n",
        "        '''\n",
        "            Input:\n",
        "                states: (B, state_size=4)\n",
        "                lstm_states: (B, 1, hidden_size=32)\n",
        "        '''\n",
        "        net = F.relu(self.fc(states)) # (B, state_size=4)\n",
        "        net = net.view(1, -1, 64) # (1, timestep=B, state_size=4)\n",
        "        net, lstm_states = self.lstm(net, lstm_states) # (B, 1, hidden_size=32), lstm_states\n",
        "        net = net.view(-1, 1, 32) # debug\n",
        "        return self.fc_v(net.squeeze(1))\n",
        "      \n",
        "    def fit(self, sample): # samples: [(s1, a1, r1), (s2, a2, r2), ...]\n",
        "        (s, a, r, ns, done_mask, old_probs, (h, c)) = sample\n",
        "        empty_lstm_state = (torch.zeros((1, 1, 32), dtype=torch.float).to(device), torch.zeros((1, 1, 32), dtype=torch.float).to(device))\n",
        "        lstm_state = (h.detach(), c.detach())\n",
        "        rewards = r / reward_div # (B, num_rollouts)\n",
        "        \n",
        "        for i in range(k_epoch):\n",
        "            td_target = (rewards + gamma * self.value(ns, lstm_state).squeeze() * done_mask).unsqueeze(1) # (num_rollouts, 1)\n",
        "            vs = self.value(s, lstm_state) # (num_rollouts, 1)\n",
        "            advantages = td_target - vs # (num_rollouts, 1)\n",
        "\n",
        "            advantages = GAE(advantages, gamma, lmbda).detach() # !!! detach the advantages\n",
        "            \n",
        "            \n",
        "#             probs, _ = self.policy(s, lstm_state, softmax_dim=1) # (num_rollouts, action_size=2) # Also work!\n",
        "            probs, _ = self.policy(s, empty_lstm_state, softmax_dim=1) # (num_rollouts, action_size=2)\n",
        "  \n",
        "            probs = probs.gather(1, a.unsqueeze(1)) # (num_rollouts, 1)\n",
        "            \n",
        "            ratio = torch.exp(torch.log(probs) - torch.log(old_probs.unsqueeze(1))) # (num_rollouts, 1) !!! tensor with size of (20 ,1) minus that of (20,) will produce (20, 20) tensor\n",
        "            surr1 = ratio * advantages\n",
        "            surr2 = torch.clamp(ratio, 1 - eps, 1 + eps) * advantages\n",
        "            \n",
        "            loss = torch.mean(-torch.min(surr1, surr2) +  F.smooth_l1_loss(vs, td_target.detach()))\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "        \n",
        "ppo = PPO().to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G21oc1HD1jTv",
        "colab_type": "text"
      },
      "source": [
        "## 5. Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "llqz3i8jJ0MF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 857
        },
        "outputId": "dcac6811-da61-44ae-d810-b8eecdde4691"
      },
      "source": [
        "score = 0.0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    sample_iter = get_sample(env, ppo)\n",
        "    for sample in sample_iter:\n",
        "        ppo.fit(sample)\n",
        "        rewards = sample[2]\n",
        "        score += sum(rewards)\n",
        "        \n",
        "    if epoch % 20 == 0:\n",
        "        print('Epoch %d || Average Score: %.6f'%(epoch, score / (epoch + 1)))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 || Average Score: 15.000000\n",
            "Epoch 20 || Average Score: 27.904762\n",
            "Epoch 40 || Average Score: 29.999998\n",
            "Epoch 60 || Average Score: 38.983604\n",
            "Epoch 80 || Average Score: 46.135803\n",
            "Epoch 100 || Average Score: 65.970299\n",
            "Epoch 120 || Average Score: 72.512390\n",
            "Epoch 140 || Average Score: 83.588654\n",
            "Epoch 160 || Average Score: 96.093170\n",
            "Epoch 180 || Average Score: 98.922653\n",
            "Epoch 200 || Average Score: 108.064674\n",
            "Epoch 220 || Average Score: 116.058830\n",
            "Epoch 240 || Average Score: 121.356850\n",
            "Epoch 260 || Average Score: 125.390800\n",
            "Epoch 280 || Average Score: 127.925262\n",
            "Epoch 300 || Average Score: 129.617935\n",
            "Epoch 320 || Average Score: 132.392517\n",
            "Epoch 340 || Average Score: 133.835770\n",
            "Epoch 360 || Average Score: 134.318558\n",
            "Epoch 380 || Average Score: 135.763779\n",
            "Epoch 400 || Average Score: 138.456360\n",
            "Epoch 420 || Average Score: 136.372925\n",
            "Epoch 440 || Average Score: 137.934250\n",
            "Epoch 460 || Average Score: 140.121475\n",
            "Epoch 480 || Average Score: 142.465698\n",
            "Epoch 500 || Average Score: 144.762482\n",
            "Epoch 520 || Average Score: 146.230316\n",
            "Epoch 540 || Average Score: 147.829941\n",
            "Epoch 560 || Average Score: 149.411758\n",
            "Epoch 580 || Average Score: 148.468155\n",
            "Epoch 600 || Average Score: 148.354401\n",
            "Epoch 620 || Average Score: 148.893723\n",
            "Epoch 640 || Average Score: 147.065521\n",
            "Epoch 660 || Average Score: 148.275330\n",
            "Epoch 680 || Average Score: 149.674011\n",
            "Epoch 700 || Average Score: 151.098434\n",
            "Epoch 720 || Average Score: 151.938980\n",
            "Epoch 740 || Average Score: 153.236160\n",
            "Epoch 760 || Average Score: 154.465179\n",
            "Epoch 780 || Average Score: 155.631241\n",
            "Epoch 800 || Average Score: 156.373276\n",
            "Epoch 820 || Average Score: 157.272842\n",
            "Epoch 840 || Average Score: 157.034485\n",
            "Epoch 860 || Average Score: 156.831589\n",
            "Epoch 880 || Average Score: 157.811584\n",
            "Epoch 900 || Average Score: 158.748062\n",
            "Epoch 920 || Average Score: 158.444092\n",
            "Epoch 940 || Average Score: 157.866104\n",
            "Epoch 960 || Average Score: 158.659744\n",
            "Epoch 980 || Average Score: 159.315994\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ADyfFFQ33njP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}