{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from gym.spaces import Discrete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create snake game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SnakeEnv(gym.Env):\n",
    "    SIZE=100\n",
    "  \n",
    "    def __init__(self, ladder_num, dices):\n",
    "        self.ladder_num = ladder_num\n",
    "        self.dices = dices\n",
    "        self.ladders = dict(np.random.randint(1, self.SIZE, size=(self.ladder_num, 2)))\n",
    "        self.observation_space = Discrete(self.SIZE + 1) # 101 !!!\n",
    "        self.action_space = Discrete(len(dices))\n",
    "\n",
    "        for k,v in list(self.ladders.items()):\n",
    "            self.ladders[v] = k\n",
    "        self.pos = 1\n",
    "\n",
    "    def reset(self):\n",
    "        self.pos = 1\n",
    "        return self.pos\n",
    "\n",
    "    def step(self, a):\n",
    "        step = np.random.randint(1, self.dices[a] + 1)\n",
    "        self.pos += step\n",
    "        if self.pos == 100:\n",
    "            return 100, 100, 1, {}\n",
    "        elif self.pos > 100:\n",
    "            self.pos = 200 - self.pos\n",
    "\n",
    "        if self.pos in self.ladders:\n",
    "            self.pos = self.ladders[self.pos]\n",
    "        return self.pos, -1, 0, {}\n",
    "\n",
    "    def reward(self, s):\n",
    "        if s == 100:\n",
    "            return 100\n",
    "        else:\n",
    "            return -1\n",
    "\n",
    "    def render(self):\n",
    "        pass\n",
    "\n",
    "def eval_game(env, policy):\n",
    "    '''\n",
    "        policy: state -> action\n",
    "    '''\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "    while True:\n",
    "        if isinstance(policy, TableAgent) or isinstance(policy, ModelFreeAgent):\n",
    "            action = policy.play(state)\n",
    "        elif isinstance(policy, list):\n",
    "            action = policy[state]\n",
    "        else:\n",
    "            raise Error('Illegal policy')\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        if done:\n",
    "            return total_reward\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build agent for snake game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TableAgent(object):\n",
    "    def __init__(self, num_ladders, env):\n",
    "        self.action_size = env.action_space.n\n",
    "        self.state_size = env.observation_space.n # 101\n",
    "\n",
    "        self.r = [env.reward(s) for s in range(self.state_size)] # (101,)\n",
    "\n",
    "        # stochastic policy: optimizing target\n",
    "        #self.pi = np.zeros((self.state_size, self.action_size)) # \\pi(a_t | s_t)\n",
    "        self.pi = np.zeros(self.state_size, dtype=np.int) # 1D is because we assume a state corresponds to only one action\n",
    "\n",
    "        ladder_move = np.vectorize(lambda x: env.ladders[x] if x in env.ladders else x)\n",
    "        \n",
    "        # transition probability\n",
    "        self.p = np.zeros((self.action_size, self.state_size, self.state_size), dtype=np.float) # p(s_{t+1} | s_t, a_t)\n",
    "        for index, dice in enumerate(env.dices): # eg: env.dices: [3, 6]\n",
    "            prob = 1.0 / dice # eg: 1/3 or 1/6\n",
    "            for src in range(1, self.state_size - 1): # 0 and 100 should not be initialized\n",
    "                #step = np.arange(1, dice + 1) # eg: [1, 2, 3] or [1, 2, 3, 4, 5, 6]\n",
    "                step = np.arange(dice)\n",
    "                dsts = src + step # a vector of dst\n",
    "                for dst in dsts:\n",
    "                    if dst > 100:\n",
    "                        dst = 200 - dst # move back\n",
    "                    dst = ladder_move(dst)\n",
    "                    self.p[index, src, dst] += prob\n",
    "\n",
    "        self.p[:, 100, 100] = 1 # If the src is 100, player can go nowhere but 100\n",
    "\n",
    "        # state-value function\n",
    "        self.value_s = np.zeros(self.state_size) # v_{\\pi}(s)\n",
    "\n",
    "        # action-value function\n",
    "        self.value_sa = np.zeros((self.state_size, self.action_size)) # q_{\\pi}(s, a)\n",
    "\n",
    "        # discount factor\n",
    "        self.gamma = 0.8\n",
    "\n",
    "    def play(self, state):\n",
    "        return self.pi[state]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Define policy iteration class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import contextmanager\n",
    "import time\n",
    "\n",
    "@contextmanager\n",
    "def timer(name):\n",
    "    start = time.time()\n",
    "    yield\n",
    "    end = time.time()\n",
    "    print('{} COST:{}'.format(name, end - start))\n",
    "\n",
    "class PolicyIteration(object):\n",
    "    @staticmethod\n",
    "    def _policy_evaluation(agent, max_iter=-1):\n",
    "        iteration = 0\n",
    "        while True:\n",
    "            new_value_s = agent.value_s.copy() # (state_size)\n",
    "            for s in range(1, agent.state_size): # 0 is invalid, because SnakeEnv defines observation_space to be 101\n",
    "                a = agent.pi[s]\n",
    "                new_value_s[s] = np.matmul(agent.p[a, s, :], agent.r + agent.gamma * agent.value_s) # (state_size)\n",
    "            diff = np.sqrt(np.sum((new_value_s - agent.value_s) ** 2))\n",
    "\n",
    "            if diff < 1e-6:\n",
    "                break\n",
    "            else:\n",
    "                agent.value_s = new_value_s\n",
    "                \n",
    "            iteration += 1\n",
    "            if iteration == max_iter:\n",
    "                break\n",
    "\n",
    "    @staticmethod\n",
    "    def _policy_improvement(agent):\n",
    "        new_policy = np.zeros_like(agent.pi) # (action_size, state_size, state_size)\n",
    "        for s in range(1, agent.state_size):\n",
    "            for a in range(agent.action_size): # iterate a !!! not a = agent.pi[s], but iteration!!!!\n",
    "                # update action-value function\n",
    "                agent.value_sa[s, a] = np.dot(agent.p[a, s, :], agent.r + agent.gamma * agent.value_s)\n",
    "            new_policy[s] = np.argmax(agent.value_sa[s, :]) # select the max action !!! not [s, a], but [s, :]\n",
    "        if np.all(np.equal(new_policy, agent.pi)):\n",
    "            return True # converge\n",
    "        else:\n",
    "            agent.pi = new_policy\n",
    "            return False # not converge\n",
    "\n",
    "    @staticmethod\n",
    "    def policy_iteration(agent, max_iter=-1): # max_iter is corresponding to policy evaluation\n",
    "        while True:\n",
    "            PolicyIteration._policy_evaluation(agent, max_iter)\n",
    "            ret = PolicyIteration._policy_improvement(agent)\n",
    "            if ret:\n",
    "                break\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def policy_iteration_time(agent, max_iter=-1):\n",
    "        while True:\n",
    "            with timer('Timer PolicyEval'):\n",
    "                PolicyIteration._policy_evaluation(agent, max_iter)\n",
    "            with timer('Timer PolicyImprove'):\n",
    "                ret = PolicyIteration._policy_improvement(agent)\n",
    "            if ret:\n",
    "                break\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No ladders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No ladders\n",
      "return: 71\n",
      "[0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print('No ladders')\n",
    "env = SnakeEnv(0, [3, 6])\n",
    "agent = TableAgent(0, env)\n",
    "PolicyIteration.policy_iteration(agent)\n",
    "print('return:', eval_game(env, agent))\n",
    "print(agent.pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random ladders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random ladders\n",
      "return: 67\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "return: 49\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "return: 93\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0]\n",
      "return: 91\n",
      "[0 1 1 1 0 0 0 1 1 1 1 1 0 0 0 0 0 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print('Random ladders')\n",
    "env = SnakeEnv(10, [3, 6])\n",
    "agent = TableAgent(0, env)\n",
    "\n",
    "# 1. all 1\n",
    "agent.pi[:] = 1\n",
    "print('return:', eval_game(env, agent))\n",
    "print(agent.pi)\n",
    "\n",
    "# 2. all 0\n",
    "agent.pi[:] = 0\n",
    "print('return:', eval_game(env, agent))\n",
    "print(agent.pi)\n",
    "\n",
    "\n",
    "# 3. [1] * 97 + [] * 3\n",
    "agent.pi[:-3] = 1\n",
    "print('return:', eval_game(env, agent))\n",
    "print(agent.pi)\n",
    "\n",
    "\n",
    "# 4. Policy Iteration\n",
    "PolicyIteration.policy_iteration(agent)\n",
    "print('return:', eval_game(env, agent))\n",
    "print(agent.pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the time policy evaluation and policy improvement runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer PolicyEval COST:0.09339094161987305\n",
      "Timer PolicyImprove COST:0.002607583999633789\n",
      "Timer PolicyEval COST:0.0632481575012207\n",
      "Timer PolicyImprove COST:0.0024428367614746094\n",
      "Timer PolicyEval COST:0.05125856399536133\n",
      "Timer PolicyImprove COST:0.0024712085723876953\n",
      "Timer PolicyEval COST:0.03414297103881836\n",
      "Timer PolicyImprove COST:0.002272367477416992\n",
      "return: 88\n",
      "[0 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1 1\n",
      " 1 1 1 0 0 0 1 0 1 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(2)\n",
    "env = SnakeEnv(10, [3, 6])\n",
    "agent = TableAgent(0, env)\n",
    "PolicyIteration.policy_iteration_time(agent)\n",
    "print('return:', eval_game(env, agent))\n",
    "print(agent.pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen from the above the results, policy evaluation takes more time than policy improvement. What if we reduce the time spent on policy evaluation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Case 1: max_iter=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer PolicyEval COST:0.05200505256652832\n",
      "Timer PolicyImprove COST:0.0023920536041259766\n",
      "Timer PolicyEval COST:0.04838132858276367\n",
      "Timer PolicyImprove COST:0.0023963451385498047\n",
      "Timer PolicyEval COST:0.05085492134094238\n",
      "Timer PolicyImprove COST:0.0030188560485839844\n",
      "Timer PolicyEval COST:0.03535056114196777\n",
      "Timer PolicyImprove COST:0.002475261688232422\n",
      "return: 88\n",
      "[0 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1 1\n",
      " 1 1 1 0 0 0 1 0 1 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(2)\n",
    "env = SnakeEnv(10, [3, 6])\n",
    "agent = TableAgent(0, env)\n",
    "PolicyIteration.policy_iteration_time(agent, max_iter=50)\n",
    "print('return:', eval_game(env, agent))\n",
    "print(agent.pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Case 2: max_iter=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer PolicyEval COST:0.010145425796508789\n",
      "Timer PolicyImprove COST:0.003003358840942383\n",
      "Timer PolicyEval COST:0.009757280349731445\n",
      "Timer PolicyImprove COST:0.003203868865966797\n",
      "Timer PolicyEval COST:0.011197090148925781\n",
      "Timer PolicyImprove COST:0.0031321048736572266\n",
      "Timer PolicyEval COST:0.01108860969543457\n",
      "Timer PolicyImprove COST:0.0023908615112304688\n",
      "return: 88\n",
      "[0 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1 1\n",
      " 1 1 1 0 0 0 1 0 1 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(2)\n",
    "env = SnakeEnv(10, [3, 6])\n",
    "agent = TableAgent(0, env)\n",
    "PolicyIteration.policy_iteration_time(agent, max_iter=10)\n",
    "print('return:', eval_game(env, agent))\n",
    "print(agent.pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Case 3: max_iter=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer PolicyEval COST:0.0011734962463378906\n",
      "Timer PolicyImprove COST:0.002821207046508789\n",
      "Timer PolicyEval COST:0.0011456012725830078\n",
      "Timer PolicyImprove COST:0.0023832321166992188\n",
      "Timer PolicyEval COST:0.001275777816772461\n",
      "Timer PolicyImprove COST:0.0023865699768066406\n",
      "Timer PolicyEval COST:0.0011174678802490234\n",
      "Timer PolicyImprove COST:0.0023479461669921875\n",
      "Timer PolicyEval COST:0.0011093616485595703\n",
      "Timer PolicyImprove COST:0.002362966537475586\n",
      "Timer PolicyEval COST:0.0011703968048095703\n",
      "Timer PolicyImprove COST:0.002497434616088867\n",
      "Timer PolicyEval COST:0.0010874271392822266\n",
      "Timer PolicyImprove COST:0.0026977062225341797\n",
      "return: 88\n",
      "[0 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1 1\n",
      " 1 1 0 0 0 0 1 1 1 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(2)\n",
    "env = SnakeEnv(10, [3, 6])\n",
    "agent = TableAgent(0, env)\n",
    "PolicyIteration.policy_iteration_time(agent, max_iter=1)\n",
    "print('return:', eval_game(env, agent))\n",
    "print(agent.pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above results are the same, indicating the times of policy evaluation does not affect the final result. In fact, The program spends more time in iterating both policy evaluation and policy improvement. Since the time spent on policy improvement is less than policy evaluation, it saves time to reduce the iterations of policy evaluation. It is called **Value Iteration**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation for Value Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Value Iteration:<br\\>\n",
    "$v(s) \\leftarrow max_a{q(s, a)}$<br\\>\n",
    "$v(s) \\leftarrow max_a{\\sum_{s^{\\prime}}{p(s^{\\prime}|s, a)[r(s, a, s^{\\prime}) + \\gamma \\tilde{v}(s^{\\prime})]}}$\n",
    "\n",
    "2. Policy Improvement:<br/>\n",
    "$q(s_t, a_t) = \\sum_{s_{t+1}}{      p(s_{t+1}|s_t, a_t)[ r_{t+1} + \\gamma v_{\\pi}(s_{t+1})  ]}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DIFFERENCES:\n",
    "1. Number of iterations for policy evalution;\n",
    "2. Ways of updating state-value function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueIteration(object):\n",
    "    @staticmethod\n",
    "    def value_iteration(agent, max_iter=-1):\n",
    "        # update state-value function\n",
    "        iteration = 0\n",
    "        while True:\n",
    "            iteration += 1\n",
    "            new_value_s = np.zeros_like(agent.value_s)\n",
    "            for s in range(1, agent.state_size): # 1 is important !!!\n",
    "                value_sas = list()\n",
    "                for a in range(agent.action_size):\n",
    "                    value_sa = np.dot(agent.p[a, s, :], agent.r + agent.gamma * agent.value_s)\n",
    "                    value_sas.append(value_sa)\n",
    "                new_value_s[s] = max(value_sas) # int or float???\n",
    "            diff = np.sqrt(np.sum((agent.value_s - new_value_s) ** 2))\n",
    "            if diff < 1e-6 or iteration == max_iter:\n",
    "                break\n",
    "            else:\n",
    "                agent.value_s = new_value_s # update agent's state-value function !!!\n",
    "        # update action-value function\n",
    "        for s in range(1, agent.state_size): # 1 is important !!!\n",
    "            for a in range(agent.action_size):\n",
    "                agent.value_sa[s, a] = np.dot(agent.p[a, s, :], agent.r + agent.gamma * agent.value_s)\n",
    "            agent.pi[s] = np.argmax(agent.value_sa[s, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value Iteration\n",
      "return: 92\n",
      "[0 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0\n",
      " 0 0 0 0 1 1 1 1 0 0 0 0 1 1 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0]\n",
      "ValueIteration COST:0.1954517364501953\n",
      "\n",
      "Policy Iteration 10 iter\n",
      "return: 92\n",
      "[0 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0\n",
      " 0 0 0 0 1 1 1 1 0 0 0 0 1 1 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0]\n",
      "PolicyIteration 10 iter COST:0.05167269706726074\n",
      "\n",
      "Policy Iteration 1 iter\n",
      "return: 92\n",
      "[0 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0\n",
      " 0 0 0 0 1 1 1 1 0 0 0 0 1 1 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0]\n",
      "PolicyIteration 1 iter COST:0.022751808166503906\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "env = SnakeEnv(10, [3, 6])\n",
    "agent = TableAgent(0, env)\n",
    "\n",
    "print('Value Iteration')\n",
    "with timer('ValueIteration'):\n",
    "    ValueIteration.value_iteration(agent)\n",
    "    print('return:', eval_game(env, agent))\n",
    "    print(agent.pi)\n",
    "\n",
    "print()\n",
    "np.random.seed(0)\n",
    "env = SnakeEnv(10, [3, 6])\n",
    "agent = TableAgent(0, env)\n",
    "print('Policy Iteration 10 iter')\n",
    "with timer('PolicyIteration 10 iter'):\n",
    "    PolicyIteration.policy_iteration(agent, max_iter=10)\n",
    "    print('return:', eval_game(env, agent))\n",
    "    print(agent.pi)\n",
    "    \n",
    "print()\n",
    "np.random.seed(0)\n",
    "env = SnakeEnv(10, [3, 6])\n",
    "agent = TableAgent(0, env)\n",
    "print('Policy Iteration 1 iter')\n",
    "with timer('PolicyIteration 1 iter'):\n",
    "    PolicyIteration.policy_iteration(agent, max_iter=1)\n",
    "    print('return:', eval_game(env, agent))\n",
    "    print(agent.pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalized Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneralizedPolicyIteration(object):\n",
    "    @staticmethod\n",
    "    def generalized_policy_iteration(agent):\n",
    "        ValueIteration.value_iteration(agent, 10)\n",
    "        PolicyIteration.policy_iteration(agent, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value Iteration\n",
      "return: 92\n",
      "[0 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0\n",
      " 0 0 0 0 1 1 1 1 0 0 0 0 1 1 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0]\n",
      "ValueIteration COST:0.030349254608154297\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "env = SnakeEnv(10, [3, 6])\n",
    "agent = TableAgent(0, env)\n",
    "\n",
    "print('Value Iteration')\n",
    "with timer('ValueIteration'):\n",
    "    GeneralizedPolicyIteration.generalized_policy_iteration(agent)\n",
    "    print('return:', eval_game(env, agent))\n",
    "    print(agent.pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
