{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "proximal_policy_optimization.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Sfm6FdR4hDz",
        "colab_type": "text"
      },
      "source": [
        "# Proximal Policy Optimization\n",
        "\n",
        "paper: [Proximal Policy Optimization Algorithms](https://arxiv.org/pdf/1707.06347.pdf)\n",
        "\n",
        "avoid updating policy network too much\n",
        "\n",
        "## Loss\n",
        "\n",
        "### 1. Trust Region Objective\n",
        "\n",
        "### 2. Clipped Surrogate Objective (our code)\n",
        "\n",
        "$$r_t(\\theta) = \\frac{\\pi_{\\theta} (a_t|s_t) }{  \\pi_{\\theta_{old}} (a_t|s_t) }$$\n",
        "\n",
        "$$L^{CLIP}(\\theta) = \\hat{\\mathbb{E}}_t[   \\text{min}(r_t(\\theta) \\hat{A}_t, \\text{clip}(r_t(\\theta), 1 - \\epsilon, 1 + \\epsilon) \\hat{A}_t )   ]$$\n",
        "\n",
        "Discourage policy change if it is outside our comfort zone.\n",
        "\n",
        "Note that $min$ is element-wise and the target is $ratio * A$. \n",
        "\n",
        "\n",
        "## Generalized Advantage Estimation (GAE)\n",
        "Blend Monte Carlo and TD together, see details [here](https://medium.com/@jonathan_hui/rl-policy-gradients-explained-advanced-topic-20c2b81a9a8b)\n",
        "\n",
        "The final advantage function for GAE is\n",
        "$$\\hat{A}_t^{GAE(\\gamma, \\lambda)} := (1 - \\lambda) ( \\hat{A_t}^{(1)} + \\lambda \\hat{A_t}^{(2)} + \\lambda^2 \\hat{A_t}^{(3)} + ...  )$$\n",
        "\n",
        "Both of n-step return and $\\lambda$-return trade off between bias and variance of the estimator.\n",
        "\n",
        "\n",
        "When $\\lambda$ is 1, it is Monte Carlo. When $\\lambda$ is 0, it is TD with one step look ahead.\n",
        "\n",
        "$$\\text{GAE}(\\gamma, 0): \\hat{A}_t := \\delta_t = r_t + \\gamma V(s_{t+1}) - V(s_t)$$\n",
        "\n",
        "$$\\text{GAE}(\\gamma, 1): \\hat{A}_t := \\sum_{l=0}^{\\infty}{\\gamma^l \\delta_{t+l}} = \\sum_{l=0}^{\\infty}{\\gamma^l r_{t+1}} - V(s_t)$$\n",
        "\n",
        "\n",
        "\n",
        "### 1. n-step return\n",
        "\n",
        "See supplementary material section on page 143:15 [here](https://arxiv.org/pdf/1804.02717.pdf)\n",
        "\n",
        "N-step return is referred to $R_t^{n} = \\sum_{l=0}^{n-1} { \\gamma^l r_{t+l}  + \\gamma ^n V(s_{t+n}) }$. N-step return provides lower variance at the cost of introducing some bias. The n-step return can be computed  by truncating the sum of returns after n steps.\n",
        "\n",
        "+ 1-step return is $R_t^{1} =  r_{t}  + \\gamma V(s_{t+1}) $ is commonly used in Q-Learning. 1-step return provides a **biased** but **lower variance** estimator.\n",
        "\n",
        "+ Monte Carlo return is referred to $R_t^{\\infty} = \\sum_{l=0}^{T-t} { \\gamma ^l r_{t+l} }$. Monte Carlo return provides an **unbiased** sample of the expected return at the given step, but results in a **high variance**.\n",
        "\n",
        "+ n acts as a trade-off between bias and variance for the value estimator\n",
        "\n",
        "\n",
        "### 2. $\\lambda$-return\n",
        "\n",
        "$$R_t{(\\lambda)} = (1-\\lambda) \\sum_{n=1}^{\\infty} {\\lambda^{n-1} R_t^n} = (1-\\lambda)(R_t + \\lambda R_t^2 + \\lambda^2 R_t^3 + ...) $$\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### GAE Codes\n",
        "```\n",
        "def GAE(advantages, gamma, lmbda):\n",
        "    gae_advantages = torch.zeros_like(advantages)\n",
        "    gae = 0\n",
        "\n",
        "    for ri in reversed(range(len(advantages))):\n",
        "        gae = gae * gamma * lmbda + advantages[ri]\n",
        "        gae_advantages[ri] = gae\n",
        "    return gae_advantages\n",
        "```\n",
        "\n",
        "The codes lead to the formula shown below:\n",
        "\n",
        "$$gae_t = \\delta_{t} + \\gamma \\lambda \\delta_{t+1} + \\gamma^2 \\lambda^2 \\delta_{t+2} + ... = \\delta_t + \\gamma \\lambda gae_{t+1} $$\n",
        "\n",
        "\n",
        "## Points\n",
        "+ Based on Advantage Actor-Critic, which means PPO trains on partial sample, etc\n",
        "+ The $\\text{log}_{\\pi_{\\theta}}(a_t|s_t)$ in the loss function is replaced by ratio, denoted as $r_t(\\theta)$\n",
        "+ One partial sample is trained several times and a new policy is obtained for each training. Then the new policy is compared with the old policy\n",
        "\n",
        "## Questions\n",
        "1. a/b == exp(log(a)-log(b)), why do we bother to use log and exp?\n",
        "\n",
        "2. Why should we select the minimum instead of simply clamping the result?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_LwZhHZv1Yl",
        "colab_type": "text"
      },
      "source": [
        "## 1. Import packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k71NQZ664P2o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.distributions import Categorical\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zU8vgPOsv25p",
        "colab_type": "text"
      },
      "source": [
        "## 2. Define constants"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8wBrx3vFvlBE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gamma = 0.98\n",
        "lmbda = 0.95\n",
        "num_epochs = 3000\n",
        "num_rollouts = 20\n",
        "reward_div = 100\n",
        "k_epoch = 3\n",
        "eps = 0.1\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YW4eN3cPv4-M",
        "colab_type": "text"
      },
      "source": [
        "## 3. Prepare data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-nz1eeDpvuQX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env = gym.make(\"CartPole-v0\")\n",
        "\n",
        "def get_sample(env, policy):\n",
        "    done = False\n",
        "    s = env.reset() # (state_size, )\n",
        "    while not done:\n",
        "        ss, aa, rr, s_primes, done_masks = list(), list(), list(), list(), list()\n",
        "        probs = list()\n",
        "        for t in range(num_rollouts):\n",
        "            a = policy.sample_action(torch.Tensor(s).to(device))\n",
        "            s_prime, r, done, _ = env.step(a) # a is 0 or 1\n",
        "            ss.append(s)\n",
        "            aa.append(a)\n",
        "            rr.append(r)\n",
        "            s_primes.append(s_prime)\n",
        "            done_mask = 0.0 if done else 1.0\n",
        "            done_masks.append(done_mask)\n",
        "            probs.append(policy.policy(torch.Tensor(s).to(device))[a])\n",
        "            s = s_prime\n",
        "            if done:\n",
        "                break\n",
        "                \n",
        "        sample = (torch.Tensor(ss).to(device), torch.LongTensor(aa).to(device), torch.Tensor(rr).to(device), torch.Tensor(s_primes).to(device), torch.Tensor(done_masks).to(device), torch.Tensor(probs).to(device))\n",
        "        yield sample"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FhCY1D9lv61p",
        "colab_type": "text"
      },
      "source": [
        "## 4. Build model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YTZwP4d9vxaw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def GAE(advantages, gamma, lmbda):\n",
        "    gae_advantages = torch.zeros_like(advantages)\n",
        "    gae = 0\n",
        "\n",
        "    for ri in reversed(range(len(advantages))):\n",
        "        gae = gae * gamma * lmbda + advantages[ri]\n",
        "        gae_advantages[ri] = gae\n",
        "    return gae_advantages\n",
        "\n",
        "\n",
        "class PPO(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(PPO, self).__init__()\n",
        "        \n",
        "        self.fc1 = nn.Linear(4, 256)\n",
        "        self.fc_pi = nn.Linear(256, 2)\n",
        "        self.fc_v = nn.Linear(256, 1)\n",
        "        \n",
        "        self.optimizer = torch.optim.Adam(self.parameters(), lr=0.0005, betas=(0.9, 0.99))\n",
        "\n",
        "    def policy(self, state, softmax_dim=0):\n",
        "        net = F.relu(self.fc1(state)) # (B, 256) # !!! Do not forget ReLU\n",
        "        net = self.fc_pi(net) # (B, 2)\n",
        "        probs = F.softmax(net, dim=softmax_dim)\n",
        "        return probs\n",
        "        \n",
        "    def sample_action(self, state, softmax_dim=0): # state: (4,) => indicates that the fully-connected layer in PyTorch can receive inputs without batch_size\n",
        "        probs = self.policy(state)\n",
        "        m = Categorical(probs) # !!! The cpu or gpu version will influence the seed. In other words, even if we set the seed to be 2, different versions of `probs` might produce different results\n",
        "        a_pred = m.sample().item()\n",
        "        return a_pred # (predicted action: 0 or 1, log of probability of current action)\n",
        "\n",
        "    def value(self, state):\n",
        "        net = F.relu(self.fc1(state)) # !!! Do not forget ReLU\n",
        "        return self.fc_v(net)\n",
        "      \n",
        "    def fit(self, sample): # samples: [(s1, a1, r1), (s2, a2, r2), ...]\n",
        "        (s, a, r, ns, done_mask, old_probs) = sample\n",
        "        rewards = r / reward_div # (B, num_rollouts)\n",
        "        \n",
        "        for i in range(k_epoch):\n",
        "            td_target = (rewards + gamma * self.value(ns).squeeze() * done_mask).unsqueeze(1) # (num_rollouts, 1)\n",
        "            vs = self.value(s) # (num_rollouts, 1)\n",
        "            advantages = td_target - vs # (num_rollouts, 1)\n",
        "\n",
        "            advantages = GAE(advantages, gamma, lmbda).detach() # !!! detach the advantages\n",
        "            \n",
        "            \n",
        "            probs = self.policy(s, softmax_dim=1) # (num_rollouts, action_size=2)\n",
        "            probs = probs.gather(1, a.unsqueeze(1)) # (num_rollouts, 1)\n",
        "            \n",
        "            ratio = torch.exp(torch.log(probs) - torch.log(old_probs.unsqueeze(1))) # (num_rollouts, 1) !!! tensor with size of (20 ,1) minus that of (20,) will produce (20, 20) tensor\n",
        "            surr1 = ratio * advantages\n",
        "            surr2 = torch.clamp(ratio, 1 - eps, 1 + eps) * advantages\n",
        "            \n",
        "            loss = torch.mean(-torch.min(surr1, surr2) +  F.smooth_l1_loss(vs, td_target.detach()))\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "        \n",
        "ppo = PPO().to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hp8vAer9v9DG",
        "colab_type": "text"
      },
      "source": [
        "## 5. Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l77F7vuhvyxE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        },
        "outputId": "b6f259e9-c4b2-4986-aea4-c0fef26a7165"
      },
      "source": [
        "score = 0.0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    sample_iter = get_sample(env, ppo)\n",
        "    for sample in sample_iter:\n",
        "        ppo.fit(sample)\n",
        "        rewards = sample[2]\n",
        "        score += sum(rewards)\n",
        "        \n",
        "    if epoch % 100 == 0:\n",
        "        print('Epoch %d || Average Score: %.6f'%(epoch, score / (epoch + 1)))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 || Average Score: 21.000000\n",
            "Epoch 100 || Average Score: 66.871284\n",
            "Epoch 200 || Average Score: 118.323380\n",
            "Epoch 300 || Average Score: 131.312286\n",
            "Epoch 400 || Average Score: 141.441406\n",
            "Epoch 500 || Average Score: 151.720566\n",
            "Epoch 600 || Average Score: 159.149750\n",
            "Epoch 700 || Average Score: 164.716125\n",
            "Epoch 800 || Average Score: 168.885147\n",
            "Epoch 900 || Average Score: 170.413986\n",
            "Epoch 1000 || Average Score: 171.368622\n",
            "Epoch 1100 || Average Score: 173.969131\n",
            "Epoch 1200 || Average Score: 175.488754\n",
            "Epoch 1300 || Average Score: 176.553421\n",
            "Epoch 1400 || Average Score: 177.571732\n",
            "Epoch 1500 || Average Score: 178.611603\n",
            "Epoch 1600 || Average Score: 176.448471\n",
            "Epoch 1700 || Average Score: 175.454437\n",
            "Epoch 1800 || Average Score: 176.618546\n",
            "Epoch 1900 || Average Score: 176.685425\n",
            "Epoch 2000 || Average Score: 176.619202\n",
            "Epoch 2100 || Average Score: 177.288437\n",
            "Epoch 2200 || Average Score: 176.102219\n",
            "Epoch 2300 || Average Score: 176.391129\n",
            "Epoch 2400 || Average Score: 177.010406\n",
            "Epoch 2500 || Average Score: 177.370651\n",
            "Epoch 2600 || Average Score: 177.786621\n",
            "Epoch 2700 || Average Score: 177.898560\n",
            "Epoch 2800 || Average Score: 177.389862\n",
            "Epoch 2900 || Average Score: 177.910721\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}