{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "deep_deterministic_policy_gradients.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XdAPBZhRTqzP",
        "colab_type": "text"
      },
      "source": [
        "# Deep Deterministic Policy Gradients (DDPG)\n",
        "\n",
        "Paper: [Continuous Control With Deep Reinforcement Learning](https://arxiv.org/pdf/1509.02971.pdf)\n",
        "\n",
        "## Objective Function and Updating\n",
        "### 1. Actor\n",
        "Maximize the expected rewards.\n",
        "\n",
        "$$\\bigtriangledown_{\\theta^{\\mu}} J \\approx \\frac{1}{N} \\sum_i{  \\bigtriangledown_a   Q(s,a | \\theta^{Q})   |_{s=s_i, a=\\mu(s_i)}  \\bigtriangledown_{\\theta^{\\mu}} \\mu(s|\\theta^{\\mu})  |_{s_i}     }$$\n",
        "\n",
        "\n",
        "### 2. Critic\n",
        "MSE Loss with TD error:\n",
        "\n",
        "$$y_i = r_i  + \\gamma Q_{target}(s_{i+1},     \\mu_{target}(s_{i+1}|\\theta^{\\mu_{target}})     | \\theta^{Q_{target}})$$\n",
        "$$L = \\frac{1}{N}\\sum_i{ (y_i - Q(s_i, a_i | {\\theta}^Q))^2  }$$\n",
        "\n",
        "\n",
        "\n",
        "### 3. Update the target networks: Soft Update\n",
        "$$\\theta^{Q_{target}} \\leftarrow \\tau \\theta^Q + (1 - \\tau)\\theta^{Q_{target}}$$\n",
        "$$\\theta^{\\mu_{target}} \\leftarrow \\tau \\theta^{\\mu} + (1 - \\tau)\\theta^{\\mu_{target}}$$\n",
        "\n",
        "where $\\tau = 0.005$ for example\n",
        "## Points\n",
        "+ Ornstein-Uhlenbeck Process\n",
        "+ Update Network: soft update & update after training\n",
        "+ Replay Buffer: save **transitions** rather than one sample\n",
        "\n",
        "## Exploration\n",
        "+ For **discrete** action spaces, exploration is done via probabitlistically selecting a random action (such as $\\epsilon$-greedy or Boltzmann exploration)\n",
        "+ For **continuous** action spaces, exploration is done via adding noise to the action itself. In the DDPG, the authors use *Ornstein-Uhlenbeck Process* to add noise to the action output\n",
        "\n",
        "### Ornstein-Uhlenbeck Process\n",
        "Generates noise that is correlated with the previous noise, as to prevent the noise from canceling out or \"freezing\" the overall dynamics.\n",
        "\n",
        "$$dx_t = \\theta (\\mu - x_t) dt + \\sigma dW_t$$\n",
        "where $W_t$ denotes the Wiener Process.\n",
        "\n",
        "\n",
        "## Pendulum-v0\n",
        "+ action: (1, ). a list of a single element which is in the range of `[-2, 2]`. The example is `[-1.5]`\n",
        "+ state: (3, )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kf397SaefV6Q",
        "colab_type": "text"
      },
      "source": [
        "## 1. Import packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VoY9QmU1fRve",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "import random\n",
        "import numpy as np\n",
        "import collections\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-J1O4wT_fXHz",
        "colab_type": "text"
      },
      "source": [
        "## 2. Define constants"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-3d6RIpfSUJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gamma = 0.99\n",
        "num_epochs = 3000\n",
        "reward_div = 100\n",
        "max_buffer = 50000\n",
        "tau = 0.005\n",
        "lr_mu = 0.0005\n",
        "lr_q = 0.001\n",
        "batch_size = 32\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xn-s-NWffYfc",
        "colab_type": "text"
      },
      "source": [
        "## 3. Prepare data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z89Yl3LjfT-I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ReplayBuffer(object):\n",
        "    def __init__(self):\n",
        "        self.buffer = collections.deque(maxlen=max_buffer)\n",
        "    \n",
        "    def append(self, sample): # I store one sample in the ReplayBuffer, while the minimalRL stores 1 transition\n",
        "        self.buffer.append(sample)\n",
        "    \n",
        "    def sample(self, n):\n",
        "        return random.sample(self.buffer, n)\n",
        "      \n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "      \n",
        "      \n",
        "buffer = ReplayBuffer()\n",
        "\n",
        "class OrnsteinUhlenbeckNoise(object):\n",
        "    def __init__(self, mu):\n",
        "        self.theta, self.dt, self.sigma = 0.1, 0.01, 0.1\n",
        "        self.mu = mu\n",
        "        self.x_prev = np.zeros_like(self.mu)\n",
        "\n",
        "    def __call__(self):\n",
        "        x = self.x_prev + self.theta * (self.mu - self.x_prev) * self.dt + \\\n",
        "            self.sigma * np.sqrt(self.dt) * np.random.normal(size=self.mu.shape)\n",
        "        self.x_prev = x\n",
        "        return x\n",
        "\n",
        "ou_noise = OrnsteinUhlenbeckNoise(mu=np.zeros(1)) # initialize with zeros and same shape as actions\n",
        "\n",
        "\n",
        "env = gym.make(\"Pendulum-v0\")\n",
        "\n",
        "\n",
        "def get_sample(env, policy, buffer, max_iter=300):\n",
        "    done = False\n",
        "    s = env.reset() # (state_size, )\n",
        "\n",
        "    ss, aa, rr, s_primes, done_masks = list(), list(), list(), list(), list()\n",
        "    for t in range(max_iter):\n",
        "        a = policy.sample_action(torch.Tensor(s).to(device)) # scalar?\n",
        "        a = a.item() + ou_noise()[0] # OUNoise only relies on the previous noise\n",
        "        s_prime, r, done, _ = env.step([a]) # a is 0 or 1\n",
        "        ss.append(s)\n",
        "        aa.append(a)\n",
        "        rr.append(r)\n",
        "        s_primes.append(s_prime)\n",
        "        done_mask = 0.0 if done else 1.0\n",
        "        done_masks.append(done_mask)\n",
        "        buffer.append((s, a, r, s_prime, done_mask))\n",
        "        s = s_prime\n",
        "        if done:\n",
        "            break\n",
        "    \n",
        "    sample = (torch.Tensor(ss).to(device), torch.FloatTensor(aa).to(device), torch.Tensor(rr).to(device), torch.Tensor(s_primes).to(device), torch.Tensor(done_masks).to(device))\n",
        "    return sample"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-IN7svSofRRN",
        "colab_type": "text"
      },
      "source": [
        "## 4. Build model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "30SzdYMzTlqA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "class Mu(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Mu, self).__init__()\n",
        "        self.state2action = nn.Sequential(\n",
        "            nn.Linear(3, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "        \n",
        "        self.optimizer = torch.optim.Adam(self.parameters(), lr=lr_mu, betas=(0.9, 0.99))\n",
        "\n",
        "\n",
        "    def sample_action(self, states):\n",
        "        '''\n",
        "            states: (B, 3)\n",
        "        '''\n",
        "        actions = torch.tanh(self.state2action(states)) * 2\n",
        "        return actions\n",
        "\n",
        "class Q(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Q, self).__init__()\n",
        "        self.fc_s = nn.Linear(3, 64)\n",
        "        self.fc_a = nn.Linear(1, 64)\n",
        "        self.fc1 = nn.Linear(128, 32)\n",
        "        self.fc2 = nn.Linear(32, 1)\n",
        "        \n",
        "        self.optimizer = torch.optim.Adam(self.parameters(), lr=lr_q, betas=(0.9, 0.99))\n",
        "\n",
        "\n",
        "    def value(self, states, actions):\n",
        "        '''\n",
        "            states: (B, 3)\n",
        "            actions: (B, 1)\n",
        "        '''\n",
        "        net1 = F.relu(self.fc_s(states))\n",
        "        net2 = F.relu(self.fc_a(actions))\n",
        "        net = torch.cat([net1, net2], dim=1) # (B, 128)\n",
        "        net = F.relu(self.fc1(net)) # (B, 32)\n",
        "        return self.fc2(net) # (B, 1)\n",
        "\n",
        "def fit(mu, q, mu_target, q_target, buffer):\n",
        "\n",
        "    transitions = buffer.sample(batch_size)\n",
        "    ss, aa, rr, s_primes, done_masks = [], [], [], [], []\n",
        "\n",
        "    for transition in transitions:\n",
        "        s, a, r, s_prime, done_mask = transition\n",
        "        ss.append(s)\n",
        "        aa.append(a)\n",
        "        rr.append(r)\n",
        "        s_primes.append(s_prime)\n",
        "        done_masks.append([done_mask])\n",
        "\n",
        "    s, a, r, s_prime, done_mask = (torch.Tensor(ss).to(device), torch.FloatTensor(aa).to(device),\\\n",
        "                                   torch.Tensor(rr).to(device), torch.Tensor(s_primes).to(device),\\\n",
        "                                   torch.Tensor(done_masks).to(device))\n",
        "\n",
        "    \n",
        "    \n",
        "    # preprocess sample\n",
        "    r /= reward_div \n",
        "    a = a.view(-1, 1) # (B, 1)\n",
        "\n",
        "    q_prime = q_target.value(s_prime, mu_target.sample_action(s_prime)) # (B, 1)\n",
        "    td_target = r.view(-1, 1) + gamma * q_prime * done_mask.view(-1, 1) # (B, 1)\n",
        "    q_loss = F.smooth_l1_loss(q.value(s, a), td_target.detach())\n",
        "    q.optimizer.zero_grad()\n",
        "    q_loss.backward()\n",
        "    q.optimizer.step()\n",
        "    \n",
        "\n",
        "    mu_loss = - torch.mean(q.value(s, mu.sample_action(s)))\n",
        "    mu.optimizer.zero_grad()\n",
        "    mu_loss.backward()\n",
        "    mu.optimizer.step()\n",
        "    \n",
        "\n",
        "def soft_update(model, model_target):\n",
        "    for param, param_target in zip(model.parameters(), model_target.parameters()):\n",
        "        param_target.data.copy_(param_target.data * (1.0 - tau) + param.data * tau)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9ueANLTfbFz",
        "colab_type": "text"
      },
      "source": [
        "## 5. Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7bcxEdqC0uwP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2745
        },
        "outputId": "9110e5d6-f8af-4112-9aff-263838c9f329"
      },
      "source": [
        "q, q_target = Q().to(device), Q().to(device)\n",
        "mu, mu_target = Mu().to(device), Mu().to(device)\n",
        "q_target.load_state_dict(q.state_dict())\n",
        "mu_target.load_state_dict(mu.state_dict())\n",
        "\n",
        "\n",
        "score = 0.0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # ------------------------- Get sample ---------------------------------------------\n",
        "    sample = get_sample(env, mu, buffer)\n",
        "    rewards = sample[2]\n",
        "    score += sum(rewards)\n",
        "\n",
        "    # ------------------------- Train Q Network using sample randomly chosen from Replay Buffer ---------------------------------------------\n",
        "    if len(buffer) > 2000:\n",
        "        for i in range(10):\n",
        "            fit(mu, q, mu_target, q_target, buffer)\n",
        "            soft_update(q, q_target) # !!! Update target network soon after the training. (Different from DQN)\n",
        "            soft_update(mu, mu_target)\n",
        "\n",
        "\n",
        "    if epoch % 20 == 0:\n",
        "        print('Epoch %d || Average Score: %.6f'%(epoch, score / (epoch + 1)))        "
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 || Average Score: -1819.201416\n",
            "Epoch 20 || Average Score: -1476.492065\n",
            "Epoch 40 || Average Score: -1392.920166\n",
            "Epoch 60 || Average Score: -1378.597900\n",
            "Epoch 80 || Average Score: -1360.710571\n",
            "Epoch 100 || Average Score: -1371.217651\n",
            "Epoch 120 || Average Score: -1389.895386\n",
            "Epoch 140 || Average Score: -1382.050903\n",
            "Epoch 160 || Average Score: -1375.786377\n",
            "Epoch 180 || Average Score: -1352.145630\n",
            "Epoch 200 || Average Score: -1339.502441\n",
            "Epoch 220 || Average Score: -1336.122070\n",
            "Epoch 240 || Average Score: -1314.615356\n",
            "Epoch 260 || Average Score: -1301.329834\n",
            "Epoch 280 || Average Score: -1294.296753\n",
            "Epoch 300 || Average Score: -1301.388550\n",
            "Epoch 320 || Average Score: -1308.272583\n",
            "Epoch 340 || Average Score: -1317.923828\n",
            "Epoch 360 || Average Score: -1288.647949\n",
            "Epoch 380 || Average Score: -1256.099243\n",
            "Epoch 400 || Average Score: -1230.564209\n",
            "Epoch 420 || Average Score: -1214.916992\n",
            "Epoch 440 || Average Score: -1193.397705\n",
            "Epoch 460 || Average Score: -1178.266968\n",
            "Epoch 480 || Average Score: -1143.149292\n",
            "Epoch 500 || Average Score: -1109.602905\n",
            "Epoch 520 || Average Score: -1090.268188\n",
            "Epoch 540 || Average Score: -1057.085693\n",
            "Epoch 560 || Average Score: -1026.092529\n",
            "Epoch 580 || Average Score: -997.633362\n",
            "Epoch 600 || Average Score: -972.015564\n",
            "Epoch 620 || Average Score: -953.099365\n",
            "Epoch 640 || Average Score: -930.431824\n",
            "Epoch 660 || Average Score: -909.163940\n",
            "Epoch 680 || Average Score: -888.508179\n",
            "Epoch 700 || Average Score: -868.797119\n",
            "Epoch 720 || Average Score: -851.034180\n",
            "Epoch 740 || Average Score: -834.176025\n",
            "Epoch 760 || Average Score: -818.029602\n",
            "Epoch 780 || Average Score: -810.353638\n",
            "Epoch 800 || Average Score: -794.064758\n",
            "Epoch 820 || Average Score: -779.274719\n",
            "Epoch 840 || Average Score: -765.497253\n",
            "Epoch 860 || Average Score: -751.193665\n",
            "Epoch 880 || Average Score: -737.804688\n",
            "Epoch 900 || Average Score: -725.367126\n",
            "Epoch 920 || Average Score: -713.806152\n",
            "Epoch 940 || Average Score: -702.484985\n",
            "Epoch 960 || Average Score: -691.796387\n",
            "Epoch 980 || Average Score: -681.771301\n",
            "Epoch 1000 || Average Score: -674.014343\n",
            "Epoch 1020 || Average Score: -664.639465\n",
            "Epoch 1040 || Average Score: -654.286987\n",
            "Epoch 1060 || Average Score: -644.783264\n",
            "Epoch 1080 || Average Score: -635.286682\n",
            "Epoch 1100 || Average Score: -626.663452\n",
            "Epoch 1120 || Average Score: -618.240479\n",
            "Epoch 1140 || Average Score: -609.509583\n",
            "Epoch 1160 || Average Score: -603.800049\n",
            "Epoch 1180 || Average Score: -595.935913\n",
            "Epoch 1200 || Average Score: -588.363464\n",
            "Epoch 1220 || Average Score: -581.197205\n",
            "Epoch 1240 || Average Score: -574.543640\n",
            "Epoch 1260 || Average Score: -568.241882\n",
            "Epoch 1280 || Average Score: -562.844910\n",
            "Epoch 1300 || Average Score: -557.232971\n",
            "Epoch 1320 || Average Score: -550.714111\n",
            "Epoch 1340 || Average Score: -545.039856\n",
            "Epoch 1360 || Average Score: -540.164001\n",
            "Epoch 1380 || Average Score: -535.235718\n",
            "Epoch 1400 || Average Score: -529.469238\n",
            "Epoch 1420 || Average Score: -524.046387\n",
            "Epoch 1440 || Average Score: -519.218506\n",
            "Epoch 1460 || Average Score: -514.212830\n",
            "Epoch 1480 || Average Score: -510.100250\n",
            "Epoch 1500 || Average Score: -505.450928\n",
            "Epoch 1520 || Average Score: -500.883942\n",
            "Epoch 1540 || Average Score: -498.012665\n",
            "Epoch 1560 || Average Score: -494.292358\n",
            "Epoch 1580 || Average Score: -490.582458\n",
            "Epoch 1600 || Average Score: -486.094330\n",
            "Epoch 1620 || Average Score: -482.075592\n",
            "Epoch 1640 || Average Score: -478.167389\n",
            "Epoch 1660 || Average Score: -474.306000\n",
            "Epoch 1680 || Average Score: -470.478577\n",
            "Epoch 1700 || Average Score: -466.679962\n",
            "Epoch 1720 || Average Score: -463.064850\n",
            "Epoch 1740 || Average Score: -459.984680\n",
            "Epoch 1760 || Average Score: -456.393677\n",
            "Epoch 1780 || Average Score: -453.020111\n",
            "Epoch 1800 || Average Score: -449.939880\n",
            "Epoch 1820 || Average Score: -446.673950\n",
            "Epoch 1840 || Average Score: -443.471466\n",
            "Epoch 1860 || Average Score: -440.384888\n",
            "Epoch 1880 || Average Score: -437.433380\n",
            "Epoch 1900 || Average Score: -434.687927\n",
            "Epoch 1920 || Average Score: -432.011627\n",
            "Epoch 1940 || Average Score: -429.131470\n",
            "Epoch 1960 || Average Score: -426.509552\n",
            "Epoch 1980 || Average Score: -423.395599\n",
            "Epoch 2000 || Average Score: -421.029846\n",
            "Epoch 2020 || Average Score: -418.237091\n",
            "Epoch 2040 || Average Score: -415.629028\n",
            "Epoch 2060 || Average Score: -412.974304\n",
            "Epoch 2080 || Average Score: -410.386749\n",
            "Epoch 2100 || Average Score: -408.078888\n",
            "Epoch 2120 || Average Score: -405.572937\n",
            "Epoch 2140 || Average Score: -403.154175\n",
            "Epoch 2160 || Average Score: -400.729736\n",
            "Epoch 2180 || Average Score: -398.616455\n",
            "Epoch 2200 || Average Score: -396.390381\n",
            "Epoch 2220 || Average Score: -394.417694\n",
            "Epoch 2240 || Average Score: -392.341797\n",
            "Epoch 2260 || Average Score: -390.269440\n",
            "Epoch 2280 || Average Score: -388.397430\n",
            "Epoch 2300 || Average Score: -388.169220\n",
            "Epoch 2320 || Average Score: -385.950775\n",
            "Epoch 2340 || Average Score: -383.733856\n",
            "Epoch 2360 || Average Score: -382.225616\n",
            "Epoch 2380 || Average Score: -381.201538\n",
            "Epoch 2400 || Average Score: -382.008453\n",
            "Epoch 2420 || Average Score: -381.665161\n",
            "Epoch 2440 || Average Score: -382.157166\n",
            "Epoch 2460 || Average Score: -381.641022\n",
            "Epoch 2480 || Average Score: -379.825500\n",
            "Epoch 2500 || Average Score: -378.295593\n",
            "Epoch 2520 || Average Score: -376.727142\n",
            "Epoch 2540 || Average Score: -374.937500\n",
            "Epoch 2560 || Average Score: -373.847046\n",
            "Epoch 2580 || Average Score: -371.920624\n",
            "Epoch 2600 || Average Score: -370.096252\n",
            "Epoch 2620 || Average Score: -368.378357\n",
            "Epoch 2640 || Average Score: -366.834015\n",
            "Epoch 2660 || Average Score: -365.221619\n",
            "Epoch 2680 || Average Score: -363.364410\n",
            "Epoch 2700 || Average Score: -361.563171\n",
            "Epoch 2720 || Average Score: -360.005859\n",
            "Epoch 2740 || Average Score: -358.458130\n",
            "Epoch 2760 || Average Score: -356.774933\n",
            "Epoch 2780 || Average Score: -355.180328\n",
            "Epoch 2800 || Average Score: -353.998962\n",
            "Epoch 2820 || Average Score: -352.551331\n",
            "Epoch 2840 || Average Score: -351.058563\n",
            "Epoch 2860 || Average Score: -349.784210\n",
            "Epoch 2880 || Average Score: -348.468842\n",
            "Epoch 2900 || Average Score: -347.209473\n",
            "Epoch 2920 || Average Score: -345.700714\n",
            "Epoch 2940 || Average Score: -344.524872\n",
            "Epoch 2960 || Average Score: -343.093719\n",
            "Epoch 2980 || Average Score: -341.917786\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}