{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "acer.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTpUCsbO4I3k",
        "colab_type": "text"
      },
      "source": [
        "# Actor Critic with Experience Replay\n",
        "\n",
        "\n",
        "## Objective\n",
        "\n",
        "### 1. Actor\n",
        "\n",
        "$$\\hat{g}_t^{acer} = \\bar{\\rho}_t  \\bigtriangledown_\\theta log \\pi_\\theta (a_t|s_t) [ Q^{ret} (s_t, a_t) - V_{\\theta_v}(s_t) ] + \\mathbb{E}_{a \\sim \\pi} ( [\\frac{\\rho_t(a) - c}{\\rho_t(a)}]_{+} \\bigtriangledown_\\theta log \\pi_\\theta (a|s_t) [Q_{\\theta_v}(s_t, a) - V_{\\theta_v}(s_t)] )   $$\n",
        "\n",
        "#### Notations\n",
        "+ $\\rho_t = \\frac{\\pi(a_t|s_t)}{\\mu(a_t|s_t)}$: importance sampling term. *(B, 1)*\n",
        "\n",
        "+ $\\bar{\\rho}_t = max\\{c, \\rho_t\\}$: reduce variance. *(B, 1)*\n",
        "+ $\\pi_\\theta(a_t|s_t)$: probability of $a_t$ under $s_t$ by current policy. *(B, 1)*\n",
        "+ $log \\pi_\\theta (a|s_t)$: probability distribution of predicted actions under $s_t$ by current policy *(B, action_size)*\n",
        "\n",
        "+ $Q^{ret}(s_t, a_t)$: return of state-action value function calculated from Retrace algortihm. *(B, 1)*\n",
        "\n",
        "+ $Q_{\\theta_v}(s_t, a)$: expected values of all actions under $s_t$ given by current value network. *(B, action_size)*\n",
        "\n",
        "+ $V_{\\theta_v}(s_t)$: state value function, the expected reward of $s_t$ by current policy and value function. For example, $V_{\\theta_v}(s_t) = \\sum_{a_t} pi_\\theta(a_t|s_t) Q_{\\theta_v}(s_t, a_t)$. *(B, 1)*. Therefore, the shape of $Q_{\\theta_v}(s_t, a) - V_{\\theta_v}(s_t)$ is *(B, action_size)*\n",
        "\n",
        "+ $\\rho_t(a) =  \\frac{\\pi(a|s_t)}{\\mu(a|s_t)} = \\pi_\\theta(s_t)$:  probability distribution of predicted actions under $s_t$. Different from $\\rho_t$. *(B, action_size)*\n",
        "\n",
        "+ $ [\\frac{\\rho_t(a) - c}{\\rho_t(a)}]_{+} = min\\{ 0, \\frac{\\rho_t(a) - c}{\\rho_t(a)} \\}$: *(B, action_size)*\n",
        "\n",
        "+ $\\mathbb{E}_{a \\sim \\pi}$: math expectation. Therefore, the second item needs to be **multiplied by the probability distribution of predicted action** and **summation**.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### 2. Critic\n",
        "$$MSE(Q^{ret}(s_t, a_t), Q_{\\theta_v}(s_t, a_t))$$\n",
        "\n",
        "#### Notations\n",
        "+ $Q_{\\theta_v}(s_t, a_t)$: state-action value function *(B, 1)*\n",
        "\n",
        "### 3. Retrace Algorithm\n",
        "$$Q^{ret}(s_t, a_t) = r_t + \\gamma \\bar{\\rho}_{t+1}[  Q^{ret}(s_{t+1}, a_{t+1}) - Q_{\\theta_v}(s_{t+1}, a_{t+1})   ] + \\gamma V_{\\theta_v}(s_{t+1})$$\n",
        "\n",
        "#### Notations\n",
        "+ $r_t$: reward from environment\n",
        "\n",
        "## Importance Sampling\n",
        "*With importance sampling, we can reuse samples collected from old policy to calculate the policy gradient.*\n",
        "### 1. derivatiton\n",
        "Problems: We want to estimate the expected value of $f(x)$ where $x$ has a data distribution $p$. However, instead of sampling from $p$, we calculate the result from sampling $q$.\n",
        "\n",
        "Deviration:\n",
        "$$\\mathbb{E}_{x \\sim p(x)}[f(x)] =  \\int_x \\mathrm{p(x)} \\mathrm{f(x)} \\,\\mathrm{d}x  = \\int_x  \\mathrm{q(x)} \\frac{ \\mathrm{p(x)}}{\\mathrm{q(x)}} \\mathrm{f(x)} \\,\\mathrm{d}x  =\n",
        "\\mathbb{E}_{x \\sim q(x)}[ \\frac{p(x)}{q(x)} f(x)]  $$\n",
        "\n",
        "### 2. property\n",
        "+ unbiased\n",
        "+ $\\frac{p(x)}{q(x)}$ cannot be too large\n",
        "\n",
        "\n",
        "## Points\n",
        "+ rollout, replay buffer\n",
        "+ Retrace Algorithm to calculate return value\n",
        "+ clip importance sampling term\n",
        "+ on policy + off policy\n",
        "+ Replay Buffer saves a part of transitions as a unit. `is_first` is used to indicate the first item of the transitions\n",
        "+ `r`, `done_mask` and `is_first` can be list rather than Tensor\n",
        "\n",
        "\n",
        "## Reference\n",
        "+ Paper: [Sample Efficient Actor-Critic with Experience Replay](https://arxiv.org/pdf/1611.01224.pdf)\n",
        "+ Code: [acer.py](https://github.com/seungeunrho/minimalRL/blob/master/acer.py)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qNpGesdVfhz",
        "colab_type": "text"
      },
      "source": [
        "## 1. Import packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKo_iXCp39aU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import collections\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.distributions import Categorical\n",
        "import torch.nn.functional as F\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9yn3ihJVg_o",
        "colab_type": "text"
      },
      "source": [
        "## 2. Define constants"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VvLV-WEuU6Bz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gamma = 0.98\n",
        "num_epochs = 5000\n",
        "num_rollouts = 10\n",
        "reward_div = 100\n",
        "c = 1.0\n",
        "max_buffer = 6000\n",
        "batch_size = 4 # 4 * num_rollouts = 40 samples total\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rp2J3u1KVih_",
        "colab_type": "text"
      },
      "source": [
        "## 3. Prepare data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AOIs5Q8uU65R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env = gym.make(\"CartPole-v0\")\n",
        "\n",
        "\n",
        "class ReplayBuffer(object):\n",
        "    def __init__(self):\n",
        "        self.buffer = collections.deque(maxlen=max_buffer)\n",
        "    \n",
        "    def append(self, sample): # I store one sample in the ReplayBuffer, while the minimalRL stores 1 transition\n",
        "        self.buffer.append(sample)\n",
        "    \n",
        "    def sample(self, n, on_policy):\n",
        "        '''\n",
        "            return transitions unit (on policy, because this transitions unit is obtained from the most recent sampling using the current policy)\n",
        "              or a batch of transitions units (off policy) \n",
        "        '''\n",
        "        if on_policy:\n",
        "            transitions_batch = [self.buffer[-1]]\n",
        "        else:\n",
        "            transitions_batch = random.sample(self.buffer, n)\n",
        "        \n",
        "        ss, aa, rr, s_primes, probs, done_masks, is_firsts = [], [], [], [], [], [], []\n",
        "        for transitions in transitions_batch:\n",
        "            is_first = True\n",
        "            for s, a, r, s_prime, done_mask, prob in transitions:\n",
        "                ss.append(s)\n",
        "                aa.append(a)\n",
        "                rr.append(r)\n",
        "                s_primes.append(s_prime)\n",
        "                probs.append(prob)\n",
        "                done_masks.append(done_mask)\n",
        "                is_firsts.append(is_first)\n",
        "                is_first = False\n",
        "                \n",
        "      \n",
        "        return torch.FloatTensor(ss).to(device), torch.LongTensor(aa).to(device), torch.FloatTensor(rr).to(device), \\\n",
        "               torch.FloatTensor(s_primes).to(device), done_masks, torch.FloatTensor(probs).to(device),  is_firsts\n",
        "      \n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "      \n",
        "buffer = ReplayBuffer()\n",
        "\n",
        "\n",
        "def get_sample(env, policy, buffer):\n",
        "    '''\n",
        "        Save transitions to buffer and return rewards for computing scores\n",
        "    '''\n",
        "    done = False\n",
        "    s = env.reset() # (state_size, )\n",
        "    is_first = True\n",
        "    while not done:\n",
        "        rr = []\n",
        "        transitions = []\n",
        "        for t in range(num_rollouts):\n",
        "            a, probs = policy.sample_action(torch.Tensor(s).to(device)) # probs.shape: (B=1, action_size)\n",
        "            s_prime, r, done, _ = env.step(a) # a is 0 or 1\n",
        "            rr.append(r)\n",
        "            done_mask = 0.0 if done else 1.0\n",
        "            \n",
        "            transitions.append((s, a, r, s_prime, done_mask, probs.detach().cpu().numpy()))\n",
        "            s = s_prime\n",
        "            if done:\n",
        "                break\n",
        "        buffer.append(transitions)\n",
        "        yield rr"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WNNrY39BVkJI",
        "colab_type": "text"
      },
      "source": [
        "## 4. Build model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q2jYTnpsU71f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def GAE(advantages, gamma, lmbda):\n",
        "    gae_advantages = torch.zeros_like(advantages)\n",
        "    gae = 0\n",
        "\n",
        "    for ri in reversed(range(len(advantages))):\n",
        "        gae = gae * gamma * lmbda + advantages[ri]\n",
        "        gae_advantages[ri] = gae\n",
        "    return gae_advantages\n",
        "\n",
        "\n",
        "class ActorCritic(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ActorCritic, self).__init__()\n",
        "        \n",
        "        self.fc1 = nn.Linear(4, 256)\n",
        "        self.fc_pi = nn.Linear(256, 2)\n",
        "        self.fc_v = nn.Linear(256, 2) # !!! value network in ACER returns value for each action\n",
        "        \n",
        "        self.optimizer = torch.optim.Adam(self.parameters(), lr=0.0005, betas=(0.9, 0.99))\n",
        "\n",
        "    def policy(self, state, softmax_dim=0):\n",
        "        net = F.relu(self.fc1(state)) # (B, 256) # !!! Do not forget ReLU\n",
        "        net = self.fc_pi(net) # (B, 2)\n",
        "        probs = F.softmax(net, dim=softmax_dim)\n",
        "        return probs\n",
        "        \n",
        "    def sample_action(self, state, softmax_dim=0): # state: (4,) => indicates that the fully-connected layer in PyTorch can receive inputs without batch_size\n",
        "        probs = self.policy(state)\n",
        "        m = Categorical(probs) # !!! The cpu or gpu version will influence the seed. In other words, even if we set the seed to be 2, different versions of `probs` might produce different results\n",
        "        a_pred = m.sample().item()\n",
        "        return a_pred, probs # (predicted action: 0 or 1, log of probability of current action)\n",
        "\n",
        "    def value(self, state):\n",
        "        net = F.relu(self.fc1(state)) # !!! Do not forget ReLU\n",
        "        return self.fc_v(net)\n",
        "      \n",
        "    def fit(self, buffer, on_policy=False):\n",
        "        s, a, r, _, done_mask, probs, is_firsts = buffer.sample(batch_size, on_policy)\n",
        "\n",
        "        # -------------------- Preprocess sample ------------------------------------\n",
        "        r /= reward_div \n",
        "        a = a.view(-1, 1) # (B, 1)\n",
        "        \n",
        "        \n",
        "        # Note: \n",
        "        # `var` represents results on all actions with the sahpe of (B, action_size)\n",
        "        # `var_a` indicates result on the specific action with shape of (B, 1)\n",
        "        \n",
        "        pi = self.policy(s, softmax_dim=1) # (B, 2) $\\pi_\\theta(a|s_t)$ !!! softmax_dim is important\n",
        "        pi_a = pi.gather(1, a) # (B, 1)  $\\pi_\\theta(a_t|s_t)$\n",
        "        \n",
        "        q = self.value(s) # (B, 2) $Q_{\\theta_v}(s_t, a)$\n",
        "        q_a = q.gather(1, a) # (B, 1) $Q_{\\theta_v}(s_{t}, a_{t})$\n",
        "\n",
        "        v = (pi * q).sum(1).unsqueeze(1).detach() # (B, 1) $V_{\\theta_v}(s_t)$ !!! detach\n",
        "        \n",
        "        rho = pi.detach() / probs\n",
        "        rho_a = rho.gather(1, a) # (B, 1), $\\rho_t$\n",
        "        rho_bar = rho_a.clamp(max=c) # (B, 1) $\\bar{\\rho}_t$\n",
        "        correction_coeff = (1 - c / rho).clamp(min=0)\n",
        "\n",
        "        \n",
        "        # --------------------- Compute return using Retrace ----------------------------\n",
        "        q_rets = list() # a list of $Q^{ret}(s_t, a_t)$\n",
        "        q_ret = v[-1] * done_mask[-1]\n",
        "        for i in reversed(range(len(r))):\n",
        "            q_ret = r[i] + gamma * q_ret\n",
        "            q_rets.append(q_ret.item())\n",
        "            q_ret = rho_bar[i] * (q_ret - q_a[i]) + v[i]\n",
        "            \n",
        "            if is_firsts[i] and i != 0: # i is the first of transitions as well as the end of the iteration. Whereas i - 1 is the last transition as well as the next iteration\n",
        "                q_ret = v[i - 1] * done_mask[i - 1]\n",
        "        \n",
        "        q_rets.reverse() # !!! reverse\n",
        "        q_rets = torch.Tensor(q_rets).to(device) # (B)\n",
        "        q_rets = q_rets.unsqueeze(1) # (B, 1)\n",
        "        \n",
        "        loss1 = rho_bar * torch.log(pi_a) * (q_rets - v)\n",
        "        loss2 = correction_coeff * torch.log(pi) * pi * (q - v)\n",
        "        loss = -(loss1 + loss2).mean() + F.smooth_l1_loss(q_a, q_rets.detach())\n",
        "        \n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        \n",
        "        \n",
        "ac = ActorCritic().to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dno4TMvVmpz",
        "colab_type": "text"
      },
      "source": [
        "## 5. Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3GVg9j6U9I8",
        "colab_type": "code",
        "outputId": "b65ba010-076f-4452-c7fa-0df1ffd37ca2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        }
      },
      "source": [
        "score = 0.0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # ------------------------- Sampling ---------------------------------------------\n",
        "    for rewards in get_sample(env, ac, buffer):\n",
        "        score += sum(rewards)\n",
        "\n",
        "    # ------------------------- Train ---------------------------------------------\n",
        "    if len(buffer) > 500:\n",
        "        ac.fit(buffer, on_policy=True)\n",
        "        ac.fit(buffer, on_policy=False)\n",
        "\n",
        "    if epoch % 100 == 0:\n",
        "        print('Epoch %d || Average Score: %.6f'%(epoch, score / (epoch + 1)))\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 || Average Score: 26.000000\n",
            "Epoch 100 || Average Score: 25.227723\n",
            "Epoch 200 || Average Score: 24.208955\n",
            "Epoch 300 || Average Score: 25.275748\n",
            "Epoch 400 || Average Score: 29.561097\n",
            "Epoch 500 || Average Score: 33.333333\n",
            "Epoch 600 || Average Score: 37.392679\n",
            "Epoch 700 || Average Score: 42.335235\n",
            "Epoch 800 || Average Score: 50.314607\n",
            "Epoch 900 || Average Score: 60.534961\n",
            "Epoch 1000 || Average Score: 69.889111\n",
            "Epoch 1100 || Average Score: 76.549500\n",
            "Epoch 1200 || Average Score: 81.293922\n",
            "Epoch 1300 || Average Score: 87.966180\n",
            "Epoch 1400 || Average Score: 91.714490\n",
            "Epoch 1500 || Average Score: 94.067288\n",
            "Epoch 1600 || Average Score: 96.288570\n",
            "Epoch 1700 || Average Score: 100.193416\n",
            "Epoch 1800 || Average Score: 102.495836\n",
            "Epoch 1900 || Average Score: 104.726460\n",
            "Epoch 2000 || Average Score: 106.287356\n",
            "Epoch 2100 || Average Score: 109.752975\n",
            "Epoch 2200 || Average Score: 112.570195\n",
            "Epoch 2300 || Average Score: 114.990439\n",
            "Epoch 2400 || Average Score: 117.872553\n",
            "Epoch 2500 || Average Score: 120.744902\n",
            "Epoch 2600 || Average Score: 123.217993\n",
            "Epoch 2700 || Average Score: 124.627916\n",
            "Epoch 2800 || Average Score: 123.885398\n",
            "Epoch 2900 || Average Score: 124.746984\n",
            "Epoch 3000 || Average Score: 125.535155\n",
            "Epoch 3100 || Average Score: 125.176072\n",
            "Epoch 3200 || Average Score: 125.795689\n",
            "Epoch 3300 || Average Score: 127.483490\n",
            "Epoch 3400 || Average Score: 127.428991\n",
            "Epoch 3500 || Average Score: 127.510711\n",
            "Epoch 3600 || Average Score: 128.557901\n",
            "Epoch 3700 || Average Score: 129.671981\n",
            "Epoch 3800 || Average Score: 131.126546\n",
            "Epoch 3900 || Average Score: 131.605998\n",
            "Epoch 4000 || Average Score: 129.769058\n",
            "Epoch 4100 || Average Score: 127.566691\n",
            "Epoch 4200 || Average Score: 126.557010\n",
            "Epoch 4300 || Average Score: 127.139037\n",
            "Epoch 4400 || Average Score: 126.966826\n",
            "Epoch 4500 || Average Score: 125.999778\n",
            "Epoch 4600 || Average Score: 125.554662\n",
            "Epoch 4700 || Average Score: 124.526484\n",
            "Epoch 4800 || Average Score: 125.062695\n",
            "Epoch 4900 || Average Score: 124.976739\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edWzX-DJU-BF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}