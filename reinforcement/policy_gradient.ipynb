{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "policy_gradient.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2kc0pe1DxRe",
        "colab_type": "text"
      },
      "source": [
        "# Policy Gradient\n",
        "\n",
        "## Loss\n",
        "$$\\bigtriangledown_{\\theta}\\text{J}(\\theta) = \\frac{1}{N}\\sum_{i=1}^N{ [ \\sum_{t=0}^T{ [\\bigtriangledown_{\\theta} \\text{log} \\pi_{\\theta}(a_{i, t}|s_{i, t}) \\text{future_rewards}(t)] }] }$$ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8e3Amo9vDxJh",
        "colab_type": "text"
      },
      "source": [
        "## 1. Import packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AYwAmbJJDsYp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.distributions import Categorical\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MMeZAclqD089",
        "colab_type": "text"
      },
      "source": [
        "## 2. Define constants"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dwKRGXRuEAc5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gamma = 0.98\n",
        "num_epochs = 2000\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ES2H6lTwD3gH",
        "colab_type": "text"
      },
      "source": [
        "## 3. Prepare data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "itVSK5IBI09q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env = gym.make(\"CartPole-v0\")\n",
        "\n",
        "def discount(rewards, gamma):\n",
        "    discounted_rewards = np.zeros_like(rewards)\n",
        "    discounted_value = 0\n",
        "\n",
        "    for ri in reversed(range(len(rewards))):\n",
        "        discounted_value = discounted_value * gamma + rewards[ri]\n",
        "        discounted_rewards[ri] = discounted_value\n",
        "    return discounted_rewards\n",
        "\n",
        "def get_sample(env, policy, max_iter=500):\n",
        "    s = env.reset() # (1, 4)\n",
        "    ss, aa, rr, log_probs = list(), list(), list(), list()\n",
        "    for i in range(max_iter):\n",
        "        a, log_prob = policy.predict(s)\n",
        "        ns, r, done, _ = env.step(a) # a is 0 or 1\n",
        "        ss.append(s)\n",
        "        aa.append(a)\n",
        "        rr.append(r)\n",
        "        log_probs.append(log_prob) # log probability of the current action\n",
        "        s = ns\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    rr = discount(rr, gamma)\n",
        "    sample = [(s, a, r) for s, a, r in zip(ss, aa, rr)]\n",
        "    return sample, log_probs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTvNGh6ND9Yv",
        "colab_type": "text"
      },
      "source": [
        "## 4. Build model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9aIXd85HEElL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Policy(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Policy, self).__init__()\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(4, 128),\n",
        "            nn.Linear(128, 2),\n",
        "        )\n",
        "        \n",
        "        self.optimizer = torch.optim.Adam(self.parameters(), lr=1e-3, betas=(0.9, 0.99))\n",
        "      \n",
        "    def predict(self, state): # state: (4,) => indicates that the fully-connected layer in PyTorch can receive inputs without batch_size\n",
        "        outputs = self.fc(torch.Tensor(state).to(device)) # outputs: (1, 2)\n",
        "        probs = F.softmax(outputs) # !!! Do not use log_softmax before Categorical, since the Categorical requires the softmax result instead of the log + softmax one. Otherwise the score might decreases while training\n",
        "        a_pred = Categorical(probs).sample()\n",
        "        return a_pred.cpu().item(), torch.log(probs[a_pred]) # (predicted action: 0 or 1, log of probability of current action)\n",
        "      \n",
        "    def fit(self, sample, log_probs): # samples: [(s1, a1, r1), (s2, a2, r2), ...], log_probs: (log_prob1, log_prob2, ...)\n",
        "        loss = 0\n",
        "        for (s, a, r), log_prob in zip(sample, log_probs):\n",
        "            loss += -log_prob * r\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "policy = Policy().to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2yWKjy1I5Is",
        "colab_type": "text"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EIK0MEvgFNFR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "outputId": "18fbd61f-969e-4039-89d9-4e6fb48544c0"
      },
      "source": [
        "score = 0.0\n",
        "for epoch in range(num_epochs):\n",
        "    sample, log_probs = get_sample(env, policy)\n",
        "    policy.fit(sample, log_probs)\n",
        "    rewards = list(zip(*sample))[1]\n",
        "    score += sum(rewards)\n",
        "    if epoch % 100 == 0:\n",
        "        print('Epoch %d || Average Score: %.6f'%(epoch, score / (epoch + 1)))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  del sys.path[0]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 || Average Score: 4.000000\n",
            "Epoch 100 || Average Score: 31.099010\n",
            "Epoch 200 || Average Score: 52.044776\n",
            "Epoch 300 || Average Score: 59.000000\n",
            "Epoch 400 || Average Score: 66.256858\n",
            "Epoch 500 || Average Score: 71.273453\n",
            "Epoch 600 || Average Score: 73.442596\n",
            "Epoch 700 || Average Score: 75.724679\n",
            "Epoch 800 || Average Score: 78.500624\n",
            "Epoch 900 || Average Score: 80.441731\n",
            "Epoch 1000 || Average Score: 82.038961\n",
            "Epoch 1100 || Average Score: 83.705722\n",
            "Epoch 1200 || Average Score: 85.029975\n",
            "Epoch 1300 || Average Score: 84.876249\n",
            "Epoch 1400 || Average Score: 83.565310\n",
            "Epoch 1500 || Average Score: 83.669554\n",
            "Epoch 1600 || Average Score: 83.032480\n",
            "Epoch 1700 || Average Score: 82.449735\n",
            "Epoch 1800 || Average Score: 82.433648\n",
            "Epoch 1900 || Average Score: 83.134666\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}