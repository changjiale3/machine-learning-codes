{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: [openai-cartpole](https://github.com/kvfrans/openai-cartpole/blob/master/cartpole-policygradient.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notations\n",
    "+ num_trans: number of (s, a, r) in the `transitions`, meaning the time steps taken to finish an episode\n",
    "+ transitions: a list of (s, a, r). Note that `s` is the old state instead of that returned by `env.step(a)`   --- (num_trans, 3)\n",
    "+ reward: current reward returned by `env.step(action)`\n",
    "+ rewards: a list of reward in 1 sample.       --- (num_trans, )\n",
    "+ future_rewards: a list of future discounted reward computed from `rewards`\n",
    "+ future_rewards_pred: a list of predicted future reward by Value Network based on current state\n",
    "+ advantages: `future_rewards` - `future_rewards_pred`\n",
    "\n",
    "\n",
    "### Objectiveness\n",
    "#### Policy Network\n",
    "**Maximize** the following expected reward.\n",
    "$$\\bigtriangledown_{\\theta}\\text{J}(\\theta) = \\frac{1}{N}\\sum_{i=1}^N{ [ \\sum_{t=0}^T{ [\\bigtriangledown_{\\theta} \\text{log} \\pi_{\\theta}(a_{i, t}|s_{i, t}) \\text{advantages}(i, t)] }]  }$$\n",
    "\n",
    "#### Value Network\n",
    "Predict the future reward as accurately as possible based on current state.\n",
    "$$\\text{loss} = \\text{MSE}(\\text{future_rewards}, \\text{future_rewards_pred})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Import packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Build Policy Network & Value Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(torch.nn.Module):\n",
    "    '''\\pi_{\\theta}(s)'''\n",
    "    def __init__(self, action_size, state_size):\n",
    "        super(Policy, self).__init__()\n",
    "\n",
    "        self.action_size = action_size\n",
    "        self.state_size = state_size\n",
    "        \n",
    "        self.fc = nn.Linear(state_size, action_size)\n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=0.01)\n",
    "        \n",
    "    def forward(self, states):\n",
    "        '''\n",
    "            states: (batch_size, state_size)\n",
    "        '''\n",
    "        prob_actions = self.fc(states) # (batch_size, action_size)\n",
    "        return F.softmax(prob_actions, dim=1) # (batch_size, action_size)\n",
    "    \n",
    "    def fit(self, states, advantages, actions):\n",
    "        '''\n",
    "            states: (batch_size, state_size)\n",
    "            advantages: (batch_size, 1)\n",
    "            actions: (batch_size, ), \n",
    "            \n",
    "            Note that batch_size is equal to num_trans\n",
    "        '''\n",
    "        prob_actions = self.forward(states) # (batch_size, action_size)\n",
    "        \n",
    "        # one-hot encoding\n",
    "        def one_hot(y, nb_digits): \n",
    "            y_onehot = torch.FloatTensor(y.size(0), nb_digits).to(device)\n",
    "            y_onehot.zero_() \n",
    "            y_onehot.scatter_(1, y.unsqueeze(1), 1) \n",
    "            return y_onehot \n",
    "        actions = one_hot(actions, 2) # (batch_size, 2)\n",
    "        \n",
    "        # log{{\\pi}_{\\theta}(a_t|s_t)}\n",
    "        probs = torch.log(torch.sum(prob_actions * actions, dim=1)) # (batch_size, )\n",
    "        # The loss of this sample\n",
    "        loss = - torch.sum(probs * advantages)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "\n",
    "    \n",
    "class Value(torch.nn.Module):\n",
    "    '''estimate reward given state'''\n",
    "    def __init__(self, state_size, output_size, hidden_size=10):\n",
    "        super(Value, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(state_size, hidden_size, bias=True)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size, bias=True)\n",
    "        \n",
    "        self.loss_fn = torch.nn.MSELoss()\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=0.1)\n",
    "\n",
    "    def forward(self, states):\n",
    "        '''\n",
    "            states: (batch_size, state_size)\n",
    "        '''\n",
    "        net = F.relu(self.fc1(states)) # (batch_size, hidden_size)\n",
    "        future_rewards_pred = self.fc2(net) # (batch_size, output_size)\n",
    "        return future_rewards_pred\n",
    "    \n",
    "    def fit(self, states, future_rewards):\n",
    "        '''\n",
    "            states: (batch_size, state_size)\n",
    "            future_rewards: (batch_size, 1)\n",
    "            \n",
    "            Note that batch_size is equal to num_trans\n",
    "        '''\n",
    "        future_rewards_pred = self.forward(states) # (batch_size, 1)\n",
    "        loss = self.loss_fn(future_rewards_pred, future_rewards)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Define training processes.\n",
    "1. Get 1 sample based on Policy Network\n",
    "2. Compute future discounted rewards\n",
    "3. Compute advantages based on Value Network\n",
    "4. Train Value Network based on states & future rewards\n",
    "5. Train Policy Network based on states, actions & advantages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(env, policy, value):\n",
    "\n",
    "    state = env.reset()\n",
    "    action_size = env.action_space.n\n",
    "\n",
    "    states = list()\n",
    "    actions = list()\n",
    "    rewards = list()\n",
    "    total_reward = 0\n",
    "    \n",
    "    # --------------- Get 1 Sample ------------------\n",
    "    for _ in range(200):\n",
    "        states.append(state) # (state_size). add old state, not current state !!!\n",
    "        # get action\n",
    "        state = np.expand_dims(state, axis=0) # (batch_size=1, state_size)\n",
    "\n",
    "        prob_actions = policy(torch.Tensor(state).to(device))[0] # (action_size)\n",
    "        #prob_actions = prob_actions.data.numpy()\n",
    "        \n",
    "        action = 0 if np.random.random() < prob_actions[0] else 1 # scalar\n",
    "        \n",
    "        state, reward, done, _ = env.step(action)\n",
    "        actions.append(action) # (action_size, ), one_hot encoding\n",
    "        rewards.append(reward) # scalar\n",
    "        total_reward += reward\n",
    "        \n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    # Compute Future Discounted Reward\n",
    "    gamma = 0.97 # old gamma=0.8 produces poor result !!!\n",
    "    def discount(rewards, gamma):\n",
    "        discounted_rewards = np.zeros_like(rewards)\n",
    "        discounted_value = 0\n",
    "\n",
    "        for ri in reversed(range(len(rewards))):\n",
    "            discounted_value = discounted_value * gamma + rewards[ri]\n",
    "            discounted_rewards[ri] = discounted_value\n",
    "        return discounted_rewards\n",
    "\n",
    "    future_rewards = discount(rewards, gamma) # (num_trans,)\n",
    "    # Get (s, a, f_r) list\n",
    "    transitions = [(s, a, f_r) for s, a, f_r in zip(states, actions, future_rewards)] # not rewards !!!\n",
    "\n",
    "    # Get advantages list\n",
    "    advantages = list()\n",
    "    for index, transition in enumerate(transitions):\n",
    "        state, action, future_reward = transition\n",
    "        state = np.expand_dims(state, axis=0) # (batch_size=1, state_size)\n",
    "        # calculate estimated reward from Value Network\n",
    "        future_reward_pred = value(torch.Tensor(state).to(device)).squeeze().detach() # (batch_size=1,)\n",
    "        future_reward_pred = future_reward_pred.cpu().numpy()\n",
    "        advantages.append(future_reward - future_reward_pred) # (num_trans,)\n",
    "    \n",
    "    future_rewards = np.expand_dims(future_rewards, axis=1) # (num_trans, 1)\n",
    "    advantages = np.expand_dims(advantages, axis=1) # (num_trans, 1)\n",
    "    \n",
    "    # Convert to tensor\n",
    "    states = torch.Tensor(states).to(device)\n",
    "    future_rewards = torch.Tensor(future_rewards).to(device)\n",
    "    advantages = torch.Tensor(advantages).to(device)\n",
    "    actions = torch.LongTensor(actions).to(device)\n",
    "    \n",
    "    # train Value Network & Policy Network\n",
    "    value.fit(states, future_rewards)\n",
    "    policy.fit(states, advantages, actions)\n",
    "\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Train & Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/orris/anaconda3/lib/python3.6/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total reward 200\n",
      "205\n",
      "avg reward after training: 158.4035\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v0\")\n",
    "\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "\n",
    "policy = Policy(action_size, state_size).to(device)\n",
    "value = Value(state_size, output_size=1).to(device)\n",
    "\n",
    "for i in range(2000):\n",
    "    total_reward = run_episode(env, policy, value)\n",
    "    if total_reward == 200:\n",
    "        print('total reward 200')\n",
    "        print(i)\n",
    "        break\n",
    "\n",
    "t = 0\n",
    "for _ in range(2000):\n",
    "    total_reward = run_episode(env, policy, value)\n",
    "    t += total_reward\n",
    "print('avg reward after training:', t / 2000)\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
