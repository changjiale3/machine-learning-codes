{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "deep_q_learning.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "athqI2lytU7g",
        "colab_type": "text"
      },
      "source": [
        "# Deep Q Learning\n",
        "\n",
        "## Loss\n",
        "$$\\bigtriangledown_{\\theta}\\text{J}(\\theta) = \\frac{1}{N}\\sum_{i=1}^N{ [ \\sum_{t=0}^T{[ \\text{smooth_l1_loss}(q(s_{i, t}), r(s_{i, t}, a_{i, t}) + \\gamma q_{target}(s_{i, t+1}) ) ]}] }$$ \n",
        "\n",
        "Note that $q(s_{i, t})$ is produced by Q Network, while $q_{target}(i, t+1)$ is given by Q_target Network\n",
        "\n",
        "## Techs\n",
        "\n",
        "+ Replay Buffer: store samples(this code) or transitions(see [here](https://github.com/seungeunrho/minimalRL/blob/master/dqn.py))\n",
        "\n",
        "+ Update Q Network once each epoch, while update Q_target Network by passing the Q Network parameters every few epochs\n",
        "\n",
        "+ After the Replay Buffer collects a sufficient number of samples, start training the Q Network\n",
        "\n",
        "+ Q Learning select the actions with highest reward instead of sampling from the softmax results, because the values outputed by Q Network represents the expected future reward of the action rather than the probability of taking the action\n",
        "\n",
        "+ $\\epsilon$-greedy: $\\epsilon$ is reduced as the training progresses based on the formula: $$max\\{0.01,  0.08 - 0.01* \\frac{epoch}{200}\\}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tu7JpvKlK3UI",
        "colab_type": "text"
      },
      "source": [
        "## 1. Import packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4v24Iq4tI54",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "import random\n",
        "import collections\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wIZVp9OFK48C",
        "colab_type": "text"
      },
      "source": [
        "## 2. Define constants"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4QcUofThPc3R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gamma = 0.98\n",
        "num_epochs = 3000\n",
        "reward_div = 100\n",
        "max_buffer = 500\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AnabdF5DK63K",
        "colab_type": "text"
      },
      "source": [
        "## 3. Prepare data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4mHJEFvxLAx_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env = gym.make(\"CartPole-v0\")\n",
        "\n",
        "def get_sample(env, policy, max_iter=600, epsilon=0.01):\n",
        "    done = False\n",
        "    s = env.reset() # (state_size, )\n",
        "\n",
        "    ss, aa, rr, s_primes, done_masks = list(), list(), list(), list(), list()\n",
        "    for t in range(max_iter):\n",
        "        a = policy.sample_action(s, epsilon=epsilon)\n",
        "        s_prime, r, done, _ = env.step(a) # a is 0 or 1\n",
        "        ss.append(s)\n",
        "        aa.append(a)\n",
        "        rr.append(r)\n",
        "        s_primes.append(s_prime)\n",
        "        done_mask = 0.0 if done else 1.0\n",
        "        done_masks.append(done_mask)\n",
        "        s = s_prime\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    sample = (torch.Tensor(ss).to(device), torch.LongTensor(aa).to(device), torch.Tensor(rr).to(device), torch.Tensor(s_primes).to(device), torch.Tensor(done_masks).to(device))\n",
        "    return sample"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_UmFIEFK8ZB",
        "colab_type": "text"
      },
      "source": [
        "## 4. Build model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BqdCTnV8teVi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ReplayBuffer(object):\n",
        "    def __init__(self):\n",
        "        self.buffer = collections.deque(maxlen=max_buffer)\n",
        "    \n",
        "    def append(self, sample): # I store one sample in the ReplayBuffer, while the minimalRL stores 1 transition\n",
        "        self.buffer.append(sample)\n",
        "    \n",
        "    def sample(self):\n",
        "        return random.choice(self.buffer)\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "      \n",
        "class DQN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DQN, self).__init__()\n",
        "        self.value = nn.Sequential(\n",
        "            nn.Linear(4, 256), # states: (B, state_size=4)\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 2) # values: (B, action_size=2)\n",
        "        )\n",
        "        \n",
        "        self.optimizer = torch.optim.Adam(self.parameters(), lr=0.0002, betas=(0.9, 0.99))\n",
        "\n",
        "      \n",
        "    def sample_action(self, state, epsilon=0.01):\n",
        "        state = torch.Tensor(state).to(device)\n",
        "        values = self.value(state) # (B=1, action_size)\n",
        "        if random.random() < epsilon:\n",
        "            return random.choice([0, 1])\n",
        "        else:\n",
        "            return values.topk(1)[1].item()\n",
        "\n",
        "    def fit(self, sample, q_target):\n",
        "        (s, a, r, s_prime, done_mask) = sample\n",
        "        \n",
        "        r /= reward_div\n",
        "        \n",
        "        q_prime = q_target.value(s_prime).max(dim=1)[0]\n",
        "        \n",
        "        td_target = r + gamma * q_prime * done_mask\n",
        "        q_pred = self.value(s) # (B, action_size=2)\n",
        "        q_pred = q_pred.gather(1, a.unsqueeze(1))\n",
        "        \n",
        "        loss = F.smooth_l1_loss(q_pred, td_target.unsqueeze(1))\n",
        "        \n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Obg4qSO2K6fr",
        "colab_type": "text"
      },
      "source": [
        "## 5. Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gsrwnk93tlUH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        },
        "outputId": "bd4a9f5d-e54d-4e03-9b00-b884d6c6812c"
      },
      "source": [
        "q = DQN().to(device)\n",
        "\n",
        "q_target = DQN().to(device)\n",
        "q_target.load_state_dict(q.state_dict())\n",
        "\n",
        "buffer = ReplayBuffer()\n",
        "\n",
        "score = 0.0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # ------------------------- Get sample ---------------------------------------------\n",
        "    epsilon = max(0.01, 0.08 - 0.01* (epoch/200)) #Linear annealing from 8% to 1%\n",
        "    sample = get_sample(env, q, epsilon=epsilon)\n",
        "    rewards = sample[2]\n",
        "    score += sum(rewards)\n",
        "    \n",
        "    # ------------------------- Append sample to Replay Buffer ---------------------------------------------\n",
        "    buffer.append(sample)\n",
        "    \n",
        "    # ------------------------- Train Q Network using sample randomly chosen from Replay Buffer ---------------------------------------------\n",
        "    if len(buffer) > 40:\n",
        "        for i in range(10):\n",
        "            sample = buffer.sample()\n",
        "            q.fit(sample, q_target)\n",
        "            \n",
        "    # ------------------------- Update Q_target Network ---------------------------------------------\n",
        "    if epoch != 0 and epoch % 20 == 0:\n",
        "        # pass the parameters from q to q_target\n",
        "        q_target.load_state_dict(q.state_dict())\n",
        "\n",
        "    if epoch % 100 == 0:\n",
        "        print('Epoch %d || Average Score: %.6f'%(epoch, score / (epoch + 1)))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 || Average Score: 10.000000\n",
            "Epoch 100 || Average Score: 15.980198\n",
            "Epoch 200 || Average Score: 26.079601\n",
            "Epoch 300 || Average Score: 62.142857\n",
            "Epoch 400 || Average Score: 82.022446\n",
            "Epoch 500 || Average Score: 94.522957\n",
            "Epoch 600 || Average Score: 104.623955\n",
            "Epoch 700 || Average Score: 112.870186\n",
            "Epoch 800 || Average Score: 114.408234\n",
            "Epoch 900 || Average Score: 111.486130\n",
            "Epoch 1000 || Average Score: 116.703293\n",
            "Epoch 1100 || Average Score: 119.029976\n",
            "Epoch 1200 || Average Score: 120.970856\n",
            "Epoch 1300 || Average Score: 121.441200\n",
            "Epoch 1400 || Average Score: 119.381157\n",
            "Epoch 1500 || Average Score: 117.480347\n",
            "Epoch 1600 || Average Score: 117.738297\n",
            "Epoch 1700 || Average Score: 118.125214\n",
            "Epoch 1800 || Average Score: 118.739029\n",
            "Epoch 1900 || Average Score: 119.277222\n",
            "Epoch 2000 || Average Score: 120.924538\n",
            "Epoch 2100 || Average Score: 121.871971\n",
            "Epoch 2200 || Average Score: 122.861427\n",
            "Epoch 2300 || Average Score: 123.503693\n",
            "Epoch 2400 || Average Score: 124.530197\n",
            "Epoch 2500 || Average Score: 124.982010\n",
            "Epoch 2600 || Average Score: 125.662437\n",
            "Epoch 2700 || Average Score: 126.407631\n",
            "Epoch 2800 || Average Score: 127.696533\n",
            "Epoch 2900 || Average Score: 129.093750\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}