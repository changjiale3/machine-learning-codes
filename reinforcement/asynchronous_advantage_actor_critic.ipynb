{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "asynchronous_advantage_actor_critic.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0-n1yvnT7TK",
        "colab_type": "text"
      },
      "source": [
        "# Asynchronous Advantage Actor-Critic (A3C)\n",
        "\n",
        "## Loss\n",
        "### 1. Policy Network\n",
        "$$\\bigtriangledown_{\\theta}\\text{J}(\\theta) = \\frac{1}{N}\\sum_{i=1}^N{ [ \\sum_{t=0}^T{ [\\bigtriangledown_{\\theta} \\text{log} \\pi_{\\theta}(a_{i, t}|s_{i, t}) (r(s_{i, t}, a_{i, t}) + \\gamma v(s_{i, t+1}) - v(i, s_t))] }] }$$\n",
        "\n",
        "### 2. Value Network\n",
        "$$\\bigtriangledown_{\\theta}\\text{J}(\\theta) = \\frac{1}{N}\\sum_{i=1}^N{ [ \\sum_{t=0}^T{[ \\text{smooth_l1_loss}(v(s_{i, t}), r(s_{i, t}, a_{i, t}) + \\gamma v(s_{i, t+1}) )  ]}] }$$\n",
        " \n",
        "## Points\n",
        "+ Train 1 Actor Critic object which shares memory using pytorch multiprocessing module\n",
        "+ CPU or GPU: In colab, the code can only be run on **CPU**\n",
        "\n",
        "## Reference\n",
        "+ Paper: [Asynchronous Methods for Deep Reinforcement Learning](https://arxiv.org/pdf/1602.01783.pdf)\n",
        "+ Code: [a3c.py](https://github.com/seungeunrho/minimalRL/blob/master/a3c.py): Note that in our code the optimizer is the member of ActorCritic object"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYB1UwHdWuaM",
        "colab_type": "text"
      },
      "source": [
        "## 1. Import packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vz60_-mZT4eg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.distributions import Categorical\n",
        "import torch.nn.functional as F\n",
        "import torch.multiprocessing as mp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sP3o5Ap_WwIv",
        "colab_type": "text"
      },
      "source": [
        "## 2. Define constants"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yvCpKLYMWt_4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gamma = 0.98\n",
        "num_epochs = 3000\n",
        "num_rollouts = 5\n",
        "reward_div = 100\n",
        "num_train_processes = 3\n",
        "\n",
        "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') # report error\n",
        "device = 'cpu'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RrRjDJBlWxpq",
        "colab_type": "text"
      },
      "source": [
        "## 3. Prepare data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qr55DooZW3hE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def get_sample(env, policy):\n",
        "    done = False\n",
        "    s = env.reset() # (state_size, )\n",
        "    while not done:\n",
        "        ss, aa, rr, s_primes, done_masks = list(), list(), list(), list(), list()\n",
        "        for t in range(num_rollouts):\n",
        "            a = policy.sample_action(s)\n",
        "            s_prime, r, done, _ = env.step(a) # a is 0 or 1\n",
        "            ss.append(s)\n",
        "            aa.append(a)\n",
        "            rr.append(r)\n",
        "            s_primes.append(s_prime)\n",
        "            done_mask = 0.0 if done else 1.0\n",
        "            done_masks.append(done_mask)\n",
        "            s = s_prime\n",
        "            if done:\n",
        "                break\n",
        "                \n",
        "        sample = (torch.Tensor(ss).to(device), torch.LongTensor(aa).to(device), torch.Tensor(rr).to(device), torch.Tensor(s_primes).to(device), torch.Tensor(done_masks).to(device))\n",
        "        yield sample"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xn5nllGMWzaG",
        "colab_type": "text"
      },
      "source": [
        "## 4. Build model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLJNaXeUXD_l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ActorCritic(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ActorCritic, self).__init__()\n",
        "        \n",
        "        self.fc1 = nn.Linear(4, 256)\n",
        "        self.fc_pi = nn.Linear(256, 2)\n",
        "        self.fc_v = nn.Linear(256, 1)\n",
        "        \n",
        "        self.optimizer = torch.optim.Adam(self.parameters(), lr=0.0002, betas=(0.9, 0.99)) # !!! env should be identical to each actor critic\n",
        "        \n",
        "\n",
        "    def policy(self, state, softmax_dim=0):\n",
        "        net = F.relu(self.fc1(state)) # (B, 256)\n",
        "        probs = F.softmax(self.fc_pi(net), dim=softmax_dim)\n",
        "        return probs\n",
        "        \n",
        "    def sample_action(self, state, softmax_dim=0): # state: (4,) => indicates that the fully-connected layer in PyTorch can receive inputs without batch_size\n",
        "        state = torch.Tensor(state).to(device)\n",
        "        probs = self.policy(state)\n",
        "        m = Categorical(probs)\n",
        "        a_pred = m.sample().item()\n",
        "        return a_pred # (predicted action: 0 or 1, log of probability of current action)\n",
        "\n",
        "    def value(self, state):\n",
        "        net = F.relu(self.fc1(state))\n",
        "        return self.fc_v(net)\n",
        "      \n",
        "def train(model, rank): # samples: [(s1, a1, r1), (s2, a2, r2), ...]\n",
        "    env = gym.make(\"CartPole-v0\") # !!! env should be identical to each actor critic\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "        sample_iter = get_sample(env, model)\n",
        "        \n",
        "        for sample in sample_iter:\n",
        "            (s, a, r, ns, done_mask) = sample\n",
        "\n",
        "            r /= reward_div\n",
        "            td_target = (r + gamma * model.value(ns).squeeze() * done_mask).unsqueeze(1) # (num_rollouts, 1)\n",
        "            vs = model.value(s) # (num_rollouts, 1)\n",
        "            advantages = td_target - vs # (num_rollouts, 1)\n",
        "\n",
        "            probs = model.policy(s, softmax_dim=1) # (num_rollouts, action_size=2)\n",
        "            probs = probs.gather(1, a.unsqueeze(1)) # (num_rollouts, 1)\n",
        "            loss = torch.mean(-torch.log(probs) * advantages.detach() +  F.smooth_l1_loss(vs, td_target.detach()))\n",
        "\n",
        "            model.optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            model.optimizer.step()\n",
        "            \n",
        "    env.close()\n",
        "    print(\"Training process {} reached maximum episode.\".format(rank))\n",
        "\n",
        "\n",
        "def test(model): # samples: [(s1, a1, r1), (s2, a2, r2), ...]\n",
        "    env = gym.make(\"CartPole-v0\") # !!! env should be identity to each actor critic\n",
        "    score = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        sample_iter = get_sample(env, model)\n",
        "        for sample in sample_iter:\n",
        "            rewards = sample[2]\n",
        "            score += sum(rewards)\n",
        "\n",
        "        if epoch % 100 == 0:\n",
        "            print('Epoch %d || Average Score: %.6f'%(epoch, score / (epoch + 1)))        \n",
        "    env.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjsEItDbW1SM",
        "colab_type": "text"
      },
      "source": [
        "## 5. Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxAb0gBKhI_f",
        "colab_type": "code",
        "outputId": "f580df7c-6ebe-40b5-ede2-397cc40e9172",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 617
        }
      },
      "source": [
        "model = ActorCritic().to(device)\n",
        "model.share_memory()\n",
        "processes = list()\n",
        "for rank in range(num_train_processes + 1):\n",
        "    if rank == 0:\n",
        "        p = mp.Process(target=test, args=(model,))\n",
        "    else:\n",
        "        p = mp.Process(target=train, args=(model, rank))\n",
        "    p.start()\n",
        "    processes.append(p)\n",
        "for p in processes:\n",
        "    p.join()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 || Average Score: 23.000000\n",
            "Epoch 100 || Average Score: 23.594059\n",
            "Epoch 200 || Average Score: 54.597015\n",
            "Epoch 300 || Average Score: 97.312294\n",
            "Epoch 400 || Average Score: 115.486282\n",
            "Epoch 500 || Average Score: 130.183640\n",
            "Epoch 600 || Average Score: 140.603989\n",
            "Epoch 700 || Average Score: 145.935806\n",
            "Epoch 800 || Average Score: 152.318359\n",
            "Epoch 900 || Average Score: 156.624863\n",
            "Epoch 1000 || Average Score: 159.665329\n",
            "Epoch 1100 || Average Score: 160.255219\n",
            "Epoch 1200 || Average Score: 159.769363\n",
            "Epoch 1300 || Average Score: 160.226746\n",
            "Epoch 1400 || Average Score: 159.668808\n",
            "Epoch 1500 || Average Score: 158.459686\n",
            "Epoch 1600 || Average Score: 159.064331\n",
            "Epoch 1700 || Average Score: 158.410339\n",
            "Epoch 1800 || Average Score: 159.218216\n",
            "Epoch 1900 || Average Score: 158.640717\n",
            "Epoch 2000 || Average Score: 157.774612\n",
            "Epoch 2100 || Average Score: 156.679199\n",
            "Epoch 2200 || Average Score: 157.944122\n",
            "Epoch 2300 || Average Score: 158.612778\n",
            "Epoch 2400 || Average Score: 160.226151\n",
            "Epoch 2500 || Average Score: 157.492203\n",
            "Epoch 2600 || Average Score: 157.163788\n",
            "Epoch 2700 || Average Score: 158.713806\n",
            "Epoch 2800 || Average Score: 159.762939\n",
            "Epoch 2900 || Average Score: 159.939682\n",
            "Training process 2 reached maximum episode.\n",
            "Training process 1 reached maximum episode.\n",
            "Training process 3 reached maximum episode.\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}