{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "actor_critic.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0-n1yvnT7TK",
        "colab_type": "text"
      },
      "source": [
        "# Actor-Critic\n",
        "\n",
        "## Loss\n",
        "### Policy Network\n",
        "$$\\bigtriangledown_{\\theta}\\text{J}(\\theta) = \\frac{1}{N}\\sum_{i=1}^N{ [ \\sum_{t=0}^T{ [\\bigtriangledown_{\\theta} \\text{log} \\pi_{\\theta}(a_{i, t}|s_{i, t}) (r(s_{i, t}, a_{i, t}) + \\gamma v(s_{i, t+1}) - v(i, s_t))] }] }$$\n",
        "\n",
        "If the episode length is $\\infty$, then $v(s)$ can get infinitely large in many cases.\n",
        "\n",
        "Simple tricks: better to get rewards sooner than later + discount factor\n",
        "\n",
        "$$y_{i, t} = r(s_{i, t}, a_{i, t}) + \\gamma v(s_{i, t+1})$$\n",
        "\n",
        "where $\\gamma \\in [0, 1]$(0.99 works well)\n",
        "\n",
        "Note that we use $\\gamma v(s_{i, t+1})$ instead of $\\gamma q(s_{i, t+1},  a_{i, t+1})$ because we choose the maximum action. In other words, the Value network is essentially the maximum future reward.\n",
        "\n",
        "### Value Network\n",
        "$$\\bigtriangledown_{\\theta}\\text{J}(\\theta) = \\frac{1}{N}\\sum_{i=1}^N{ [ \\sum_{t=0}^T{[ \\text{smooth_l1_loss}(v(s_{i, t}), r(s_{i, t}, a_{i, t}) + \\gamma v(s_{i, t+1}) )  ]}] }$$\n",
        " \n",
        " \n",
        "## Coding Notes\n",
        "+ In this code, the reward should be divided by 100\n",
        "\n",
        "+ ReLU is very important\n",
        " \n",
        "+ $\\gamma v(s_{i, t+1})$ can be multiplied by (1-done). In other words, if this (s, a) pair terminates this episode, then $\\gamma v(s_{i, t+1})$ is not required.\n",
        "\n",
        "+ Update the parameters during the episode. However, the Policy Gradient updates the parameters after one episode\n",
        "\n",
        "## Questions\n",
        "\n",
        "+ Why does the reward should be multiplied by 100?\n",
        "\n",
        "+ What role does the rollout perform?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYB1UwHdWuaM",
        "colab_type": "text"
      },
      "source": [
        "## 1. Import packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vz60_-mZT4eg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.distributions import Categorical\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sP3o5Ap_WwIv",
        "colab_type": "text"
      },
      "source": [
        "## 2. Define constants"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yvCpKLYMWt_4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gamma = 0.98\n",
        "num_epochs = 10000\n",
        "num_rollouts = 5\n",
        "reward_div = 100\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RrRjDJBlWxpq",
        "colab_type": "text"
      },
      "source": [
        "## 3. Prepare data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qr55DooZW3hE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env = gym.make(\"CartPole-v0\")\n",
        "\n",
        "def get_sample(env, policy):\n",
        "    done = False\n",
        "    s = env.reset() # (state_size, )\n",
        "    while not done:\n",
        "        ss, aa, rr, nss, dones, log_probs = list(), list(), list(), list(), list(), list()\n",
        "        for t in range(num_rollouts):\n",
        "            a, log_prob = policy.predict(torch.Tensor(s).to(device))\n",
        "            ns, r, done, _ = env.step(a) # a is 0 or 1\n",
        "            ss.append(s)\n",
        "            aa.append(a)\n",
        "            rr.append(r)\n",
        "            nss.append(ns)\n",
        "            dones.append(done)\n",
        "            log_probs.append(log_prob) # log probability of the current action\n",
        "            s = ns\n",
        "            if done:\n",
        "                break\n",
        "                \n",
        "        sample = (torch.Tensor(ss).to(device), torch.LongTensor(aa).to(device), torch.Tensor(rr).to(device), torch.Tensor(nss).to(device), torch.Tensor(dones).to(device))\n",
        "        log_probs = torch.Tensor(log_probs).to(device)\n",
        "        yield sample, log_probs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xn5nllGMWzaG",
        "colab_type": "text"
      },
      "source": [
        "## 4. Build model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLJNaXeUXD_l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class ActorCritic(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ActorCritic, self).__init__()\n",
        "        \n",
        "        self.fc1 = nn.Linear(4, 256)\n",
        "        self.fc_pi = nn.Linear(256, 2)\n",
        "        self.fc_v = nn.Linear(256, 1)\n",
        "        \n",
        "        self.optimizer = torch.optim.Adam(self.parameters(), lr=0.0002, betas=(0.9, 0.99))\n",
        "\n",
        "    def policy(self, state, softmax_dim=0):\n",
        "        net = F.relu(self.fc1(state)) # (B, 256) # !!! Do not forget ReLU\n",
        "        net = self.fc_pi(net) # (B, 2)\n",
        "        probs = F.softmax(net, dim=softmax_dim)\n",
        "        return probs\n",
        "        \n",
        "    def predict(self, state, softmax_dim=0): # state: (4,) => indicates that the fully-connected layer in PyTorch can receive inputs without batch_size\n",
        "        probs = self.policy(state)\n",
        "        m = Categorical(probs) # !!! The cpu or gpu version will influence the seed. In other words, even if we set the seed to be 2, different versions of `probs` might produce different results\n",
        "        a_pred = m.sample().item()\n",
        "        return a_pred, torch.log(probs[a_pred]) # (predicted action: 0 or 1, log of probability of current action)\n",
        "\n",
        "    def value(self, state):\n",
        "        net = F.relu(self.fc1(state)) # !!! Do not forget ReLU\n",
        "        return self.fc_v(net)\n",
        "      \n",
        "    def fit(self, sample, log_probs): # samples: [(s1, a1, r1), (s2, a2, r2), ...], log_probs: (log_prob1, log_prob2, ...)\n",
        "        (s, a, r, ns, done) = sample\n",
        "        \n",
        "        r /= reward_div # !!! divide by 100 is very important\n",
        "        td_target = (r + gamma * self.value(ns).squeeze() * (1 - done)).unsqueeze(1) # (num_rollouts, 1)\n",
        "        vs = self.value(s) # (num_rollouts, 1)\n",
        "        delta = td_target - vs # (num_rollouts, 1)\n",
        "        \n",
        "        probs = self.policy(s, softmax_dim=1) # (num_rollouts, action_size=2)\n",
        "        probs = probs.gather(1, a.unsqueeze(1)) # (num_rollouts, 1)\n",
        "        loss = torch.mean(-torch.log(probs) * delta.detach() +  F.smooth_l1_loss(td_target.detach(), vs))\n",
        "        \n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        \n",
        "ac = ActorCritic().to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjsEItDbW1SM",
        "colab_type": "text"
      },
      "source": [
        "## 5. Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUzvlHRuXFRi",
        "colab_type": "code",
        "outputId": "8222a707-65d7-4166-db53-8c87e9b75efb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 954
        }
      },
      "source": [
        "score = 0.0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    sample_iter = get_sample(env, ac)\n",
        "    for sample, log_probs in sample_iter:\n",
        "        ac.fit(sample, log_probs)\n",
        "        rewards = sample[2] * reward_div\n",
        "        score += sum(rewards)\n",
        "        \n",
        "    if epoch % 100 == 0:\n",
        "        print('Epoch %d || Average Score: %.6f'%(epoch, score / (epoch + 1)))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 || Average Score: 50.000000\n",
            "Epoch 100 || Average Score: 15.366337\n",
            "Epoch 200 || Average Score: 26.328358\n",
            "Epoch 300 || Average Score: 49.136211\n",
            "Epoch 400 || Average Score: 78.668335\n",
            "Epoch 500 || Average Score: 99.351295\n",
            "Epoch 600 || Average Score: 114.339432\n",
            "Epoch 700 || Average Score: 123.582031\n",
            "Epoch 800 || Average Score: 130.641693\n",
            "Epoch 900 || Average Score: 134.974472\n",
            "Epoch 1000 || Average Score: 140.289703\n",
            "Epoch 1100 || Average Score: 145.574936\n",
            "Epoch 1200 || Average Score: 149.524567\n",
            "Epoch 1300 || Average Score: 152.730972\n",
            "Epoch 1400 || Average Score: 155.738754\n",
            "Epoch 1500 || Average Score: 158.640244\n",
            "Epoch 1600 || Average Score: 159.883835\n",
            "Epoch 1700 || Average Score: 160.951202\n",
            "Epoch 1800 || Average Score: 161.444199\n",
            "Epoch 1900 || Average Score: 162.806946\n",
            "Epoch 2000 || Average Score: 162.570221\n",
            "Epoch 2100 || Average Score: 163.102814\n",
            "Epoch 2200 || Average Score: 162.592911\n",
            "Epoch 2300 || Average Score: 162.478043\n",
            "Epoch 2400 || Average Score: 162.519363\n",
            "Epoch 2500 || Average Score: 162.505005\n",
            "Epoch 2600 || Average Score: 162.743179\n",
            "Epoch 2700 || Average Score: 163.220291\n",
            "Epoch 2800 || Average Score: 164.072479\n",
            "Epoch 2900 || Average Score: 163.360565\n",
            "Epoch 3000 || Average Score: 163.859390\n",
            "Epoch 3100 || Average Score: 163.927444\n",
            "Epoch 3200 || Average Score: 163.720093\n",
            "Epoch 3300 || Average Score: 162.506516\n",
            "Epoch 3400 || Average Score: 162.165253\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-9091d77ac059>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0msample_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mac\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_probs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msample_iter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mac\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_probs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mreward_div\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-9f1799a245eb>\u001b[0m in \u001b[0;36mget_sample\u001b[0;34m(env, policy)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maa\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_rollouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m             \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m             \u001b[0mns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# a is 0 or 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}