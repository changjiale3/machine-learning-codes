{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extending Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See details at [this](https://pytorch.org/docs/stable/notes/extending.html?highlight=extend%20autograd).\n",
    "\n",
    "See Exp example at [this](https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function\n",
    "### forward\n",
    "The `forward` function in `Function` derived class is similar to that in `torch.nn.Modules` derived class. The difference is that the first parameter must be `ctx` where we can save some parameters in `forward` function and use them later in the `backward` function.\n",
    "\n",
    "### backward\n",
    "The parameters contain ctx and $\\frac{\\partial loss}{\\partial output}$, where $loss = f(output)$ and `output` is exactly the `output` returned by `forward` method. Note that `output` can be a scalar, vector or a matrix and `loss` must be a scalar. If the `output` is a matrix, then the `grad_output` in the parameter list of `backward` method, i.e., $\\frac{\\partial loss}{\\partial output}$ is also a matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Function, gradcheck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearFunction(Function):\n",
    "\n",
    "    # Note that both forward and backward are @staticmethods\n",
    "    @staticmethod\n",
    "    # bias is an optional argument\n",
    "    def forward(ctx, input, weight, bias=None):\n",
    "        ctx.save_for_backward(input, weight, bias)\n",
    "        output = input.mm(weight)\n",
    "        if bias is not None:\n",
    "            output += bias.unsqueeze(0).expand_as(output)\n",
    "        print('FORWARD: output=', output)\n",
    "        return output\n",
    "    # This function has only a single output, so it gets only one gradient\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # This is a pattern that is very convenient - at the top of backward\n",
    "        # unpack saved_tensors and initialize all gradients w.r.t. inputs to\n",
    "        # None. Thanks to the fact that additional trailing Nones are\n",
    "        # ignored, the return statement is simple even when the function has\n",
    "        # optional inputs.\n",
    "        print('BACKWARD: gard_output=', grad_output)\n",
    "        input, weight, bias = ctx.saved_tensors\n",
    "        grad_input = grad_weight = grad_bias = None\n",
    "        if ctx.needs_input_grad[0]:\n",
    "            grad_input = grad_output.mm(weight.t())\n",
    "        if ctx.needs_input_grad[1]:\n",
    "            grad_weight = input.t().mm(grad_output)\n",
    "        if bias is not None and ctx.needs_input_grad[2]:\n",
    "            grad_bias = grad_output.sum(0).squeeze(0)\n",
    "        print('BACKWARD: gard_input=', grad_input)\n",
    "        print('BACKWARD: gard_weight=', grad_weight)\n",
    "        print('BACKWARD: gard_bias=', grad_bias)\n",
    "        return grad_input, grad_weight, grad_bias\n",
    "linear = LinearFunction.apply"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `linear(a, b)` is the `output` returned by `forward` method in LinearFunction. Since the loss is computed by simply the element-wise summation of `output`, so the `grad_output` in `backward` method equals to \n",
    "$$\n",
    "\\text{grad_output} = \n",
    " \\begin{pmatrix}\n",
    "  1 & 1 & 1 \\\\\n",
    "  1 & 1 & 1 \\\\\n",
    " \\end{pmatrix}$$.\n",
    " \n",
    " \n",
    "If $y = \\sum{A \\times B}$, then \n",
    "1. $\\frac{\\partial y}{\\partial A} = \\frac{\\partial y}{\\partial u} \\times \\frac{\\partial u}{\\partial A} = \\frac{\\partial y}{\\partial u} \\times B^T, \\text{where } u = A \\times B$\n",
    "\n",
    "2. $\\frac{\\partial y}{\\partial B} = \\frac{\\partial y}{\\partial u} \\times \\frac{\\partial u}{\\partial B} = A^T \\times \\frac{\\partial y}{\\partial u}, \\text{where } u = A \\times B$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1.],\n",
      "        [2., 3.]], dtype=torch.float64, requires_grad=True)\n",
      "tensor([[0., 1., 2.],\n",
      "        [3., 4., 5.]], dtype=torch.float64, requires_grad=True)\n",
      "FORWARD: output= tensor([[ 3.,  4.,  5.],\n",
      "        [ 9., 14., 19.]], dtype=torch.float64)\n",
      "output= tensor([[ 3.,  4.,  5.],\n",
      "        [ 9., 14., 19.]],\n",
      "       dtype=torch.float64, grad_fn=<LinearFunctionBackward>)\n",
      "BACKWARD: gard_output= tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]], dtype=torch.float64)\n",
      "BACKWARD: gard_input= tensor([[ 3., 12.],\n",
      "        [ 3., 12.]], dtype=torch.float64)\n",
      "BACKWARD: gard_weight= tensor([[2., 2., 2.],\n",
      "        [4., 4., 4.]], dtype=torch.float64)\n",
      "BACKWARD: gard_bias= None\n"
     ]
    }
   ],
   "source": [
    "#input = (torch.randn(20,20,dtype=torch.double,requires_grad=True), torch.randn(30,20,dtype=torch.double,requires_grad=True))\n",
    "a = torch.arange(4).view(2,2).double()       \n",
    "b = torch.arange(6).view(2,3).double()       \n",
    "a.requires_grad = True                      \n",
    "b.requires_grad = True                      \n",
    "\n",
    "\n",
    "print(a)\n",
    "print(b)\n",
    "output = linear(a, b)\n",
    "print('output=', output)\n",
    "loss = torch.sum(output)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gradcheck\n",
    "\n",
    "Check gradients computed via small finite differences against analytical gradients w.r.t. tensors in :attr:`inputs` that are of floating point type and with ``requires_grad=True``.\n",
    "\n",
    "If the `backward` is corresponds to the `forward`, then `gradcheck` returns True, otherwise reports error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Case 1: The correct way (We use a correct way to compute `grad_weight`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "class LinearFunction(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, weight, bias=None):\n",
    "        ctx.save_for_backward(input, weight, bias)\n",
    "        output = input.mm(weight)\n",
    "        if bias is not None:\n",
    "            output += bias.unsqueeze(0).expand_as(output)\n",
    "        return output\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input, weight, bias = ctx.saved_tensors\n",
    "        grad_input = grad_weight = grad_bias = None\n",
    "        if ctx.needs_input_grad[0]:\n",
    "            grad_input = grad_output.mm(weight.t())\n",
    "        if ctx.needs_input_grad[1]:\n",
    "            grad_weight = input.t().mm(grad_output)\n",
    "        if bias is not None and ctx.needs_input_grad[2]:\n",
    "            grad_bias = grad_output.sum(0).squeeze(0)\n",
    "        return grad_input, grad_weight, grad_bias\n",
    "linear = LinearFunction.apply\n",
    "\n",
    "test = gradcheck(linear, (a, b), eps=1e-6, atol=1e-4)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Case 2: What if the backward has problems? (We use a wrong way to compute `grad_weight`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Jacobian mismatch for output 0 with respect to input 1,\nnumerical:tensor([[0.0000, 0.0000, 0.0000, 2.0000, 0.0000, 0.0000],\n        [0.0000, 0.0000, 0.0000, 0.0000, 2.0000, 0.0000],\n        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.0000],\n        [1.0000, 0.0000, 0.0000, 3.0000, 0.0000, 0.0000],\n        [0.0000, 1.0000, 0.0000, 0.0000, 3.0000, 0.0000],\n        [0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 3.0000]],\n       dtype=torch.float64)\nanalytical:tensor([[0., 0., 0., 1., 0., 0.],\n        [0., 0., 0., 0., 1., 0.],\n        [0., 0., 0., 0., 0., 1.],\n        [2., 0., 0., 3., 0., 0.],\n        [0., 2., 0., 0., 3., 0.],\n        [0., 0., 2., 0., 0., 3.]], dtype=torch.float64)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-2e4e077c6d4b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mlinear\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLinearFunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgradcheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/autograd/gradcheck.py\u001b[0m in \u001b[0;36mgradcheck\u001b[0;34m(func, inputs, eps, atol, rtol, raise_exception)\u001b[0m\n\u001b[1;32m    212\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0matol\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrtol\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m                     return fail_test('Jacobian mismatch for output %d with respect to input %d,\\n'\n\u001b[0;32m--> 214\u001b[0;31m                                      'numerical:%s\\nanalytical:%s\\n' % (i, j, n, a))\n\u001b[0m\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mreentrant\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/autograd/gradcheck.py\u001b[0m in \u001b[0;36mfail_test\u001b[0;34m(msg)\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfail_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mraise_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Jacobian mismatch for output 0 with respect to input 1,\nnumerical:tensor([[0.0000, 0.0000, 0.0000, 2.0000, 0.0000, 0.0000],\n        [0.0000, 0.0000, 0.0000, 0.0000, 2.0000, 0.0000],\n        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.0000],\n        [1.0000, 0.0000, 0.0000, 3.0000, 0.0000, 0.0000],\n        [0.0000, 1.0000, 0.0000, 0.0000, 3.0000, 0.0000],\n        [0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 3.0000]],\n       dtype=torch.float64)\nanalytical:tensor([[0., 0., 0., 1., 0., 0.],\n        [0., 0., 0., 0., 1., 0.],\n        [0., 0., 0., 0., 0., 1.],\n        [2., 0., 0., 3., 0., 0.],\n        [0., 2., 0., 0., 3., 0.],\n        [0., 0., 2., 0., 0., 3.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "class LinearFunction(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, weight, bias=None):\n",
    "        ctx.save_for_backward(input, weight, bias)\n",
    "        output = input.mm(weight)\n",
    "        if bias is not None:\n",
    "            output += bias.unsqueeze(0).expand_as(output)\n",
    "        return output\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input, weight, bias = ctx.saved_tensors\n",
    "        grad_input = grad_weight = grad_bias = None\n",
    "        if ctx.needs_input_grad[0]:\n",
    "            grad_input = grad_output.mm(weight.t())\n",
    "        if ctx.needs_input_grad[1]:\n",
    "            grad_weight = input.mm(grad_output)\n",
    "        if bias is not None and ctx.needs_input_grad[2]:\n",
    "            grad_bias = grad_output.sum(0).squeeze(0)\n",
    "        return grad_input, grad_weight, grad_bias\n",
    "linear = LinearFunction.apply\n",
    "\n",
    "test = gradcheck(linear, (a, b), eps=1e-6, atol=1e-4)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: $y = e^x$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "output = tensor(7.3891, dtype=torch.float64, grad_fn=<ExpBackward>)\n",
      "tensor(7.3891, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "class Exp(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        output = input.exp()\n",
    "        ctx.save_for_backward(output)\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        grad_input = None\n",
    "        output, = ctx.saved_tensors\n",
    "        grad_input = grad_output * output\n",
    "        return grad_input\n",
    "\n",
    "exp = Exp.apply\n",
    "\n",
    "x = torch.Tensor([2]).squeeze().double()\n",
    "x.requires_grad = True\n",
    "test = gradcheck(exp, (x,), eps=1e-6, atol=1e-4)\n",
    "print(test)\n",
    "output = exp(x)\n",
    "print('output =', output)\n",
    "output.backward()\n",
    "print(x.grad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
