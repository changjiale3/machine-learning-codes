{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def dynamic_rnn(cell, inputs, sequence_length=None, initial_state=None, dtype=None):\n",
    "    '''\n",
    "        Inputs:\n",
    "            cell: torch.nn.LSTMCell instance\n",
    "            inputs: (batch_size, max_timestep, input_size)\n",
    "            sequence_length: (batch_size,)\n",
    "            initial_state: a tuple of (hidden_state, cell_state)\n",
    "        Outputs:\n",
    "            outputs: (batch_size, max_timestep, hidden_size)\n",
    "            final_states(NOT SUPPORT): a tuple of (hidden_state, cell_state) which are the final states of the cell\n",
    "    '''\n",
    "    def sort_batch(data, lengths):\n",
    "        '''\n",
    "            data: (batch_size, ?)\n",
    "            lengths: (batch_size,)\n",
    "        '''\n",
    "        sorted_indices, sorted_lengths = zip(*sorted(enumerate(lengths), key=lambda x: x[1], reverse=True))\n",
    "        sorted_indices = list(sorted_indices)\n",
    "        sorted_data = data[sorted_indices]\n",
    "        return sorted_data, sorted_lengths, sorted_indices\n",
    "\n",
    "    dtype = inputs.dtype\n",
    "    device = inputs.device\n",
    "    if sequence_length is None:\n",
    "        sequence_length = torch.LongTensor([inputs.shape[1]]).to(deviec)\n",
    "        \n",
    "    sorted_inputs, sorted_lengths, sorted_indices = sort_batch(inputs, sequence_length)\n",
    "    \n",
    "    decoder_lengths = [length - 1 for length in sorted_lengths]\n",
    "\n",
    "    sorted_outputs = torch.zeros((inputs.shape[0], inputs.shape[1], cell.hidden_size), dtype=dtype).to(device)\n",
    "    outputs = torch.zeros((inputs.shape[0], inputs.shape[1], cell.hidden_size), dtype=dtype).to(device)\n",
    "\n",
    "\n",
    "    h, c = None, None\n",
    "    for step in range(sorted_lengths[0]):\n",
    "        curr_batch_size = sum([l > step for l in sorted_lengths])\n",
    "        #sorted_inputs = sorted_inputs[:curr_batch_size, step, :] # (curr_batch_size, timesteps, input_size)\n",
    "        curr_inputs = sorted_inputs[:curr_batch_size, step, :] # (batch_size, input_size)\n",
    "\n",
    "        if h is None or c is None:\n",
    "            h, c = cell(curr_inputs, None) # (curr_batch_size, hidden_size)\n",
    "        else:\n",
    "            h, c = h[:curr_batch_size], c[:curr_batch_size] # (curr_batch_size, hidden_size)\n",
    "            h, c = cell(curr_inputs, (h, c)) # (curr_batch_size, hidden_size)\n",
    "\n",
    "        sorted_outputs[:curr_batch_size, step, :] = h\n",
    "\n",
    "    # sort back\n",
    "    outputs[sorted_indices] = sorted_outputs\n",
    "\n",
    "    return outputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 5, 2])\n",
      "tensor([[[-0.8641,  1.3578],\n",
      "         [ 0.3086,  0.9989],\n",
      "         [ 0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000]],\n",
      "\n",
      "        [[-0.3247, -0.5936],\n",
      "         [ 0.4447, -0.7139],\n",
      "         [ 0.1393, -0.2321],\n",
      "         [ 0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000]],\n",
      "\n",
      "        [[-0.9856, -1.9107],\n",
      "         [-0.7021,  1.3406],\n",
      "         [-1.4402,  1.8186],\n",
      "         [-1.5619,  1.6430],\n",
      "         [ 0.0000,  0.0000]],\n",
      "\n",
      "        [[-0.7612, -0.6508],\n",
      "         [ 0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000]]])\n",
      "------------------------------\n",
      "sequence_length:\n",
      "tensor([2, 3, 4, 1])\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.randn(4, 5, 2)\n",
    "sequence_length = torch.LongTensor([2, 3, 4, 1])\n",
    "\n",
    "for i in range(inputs.shape[0]):\n",
    "    inputs[i, sequence_length[i]:, :] = 0\n",
    "    \n",
    "lstm = torch.nn.LSTMCell(input_size=2, hidden_size=4)\n",
    "\n",
    "print('inputs:')\n",
    "print(inputs.shape)\n",
    "print(inputs)\n",
    "\n",
    "print('-'*30)\n",
    "print('sequence_length:')\n",
    "print(sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 5, 4])\n",
      "tensor([[[-0.0921,  0.3016,  0.1301,  0.1123],\n",
      "         [-0.0995,  0.3855,  0.0897,  0.1728],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[-0.0688,  0.1002, -0.0037, -0.0198],\n",
      "         [ 0.0235,  0.0934, -0.0508, -0.0198],\n",
      "         [-0.0286,  0.1563, -0.0523,  0.0247],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[-0.1092,  0.0302, -0.0278, -0.1158],\n",
      "         [-0.1288,  0.3310,  0.0633,  0.0567],\n",
      "         [-0.1283,  0.4139,  0.1793,  0.1755],\n",
      "         [-0.1458,  0.4177,  0.2437,  0.2406],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[-0.1176,  0.1075,  0.0113, -0.0338],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000]]], grad_fn=<PutBackward>)\n"
     ]
    }
   ],
   "source": [
    "outputs = dynamic_rnn(lstm, inputs, sequence_length=sequence_length)\n",
    "\n",
    "print(outputs.shape)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 2])\n",
      "tensor([[[-0.0921,  0.3016,  0.1301,  0.1123],\n",
      "         [-0.0995,  0.3855,  0.0897,  0.1728]]], grad_fn=<CopySlices>)\n",
      "torch.Size([1, 3, 2])\n",
      "tensor([[[-0.0688,  0.1002, -0.0037, -0.0198],\n",
      "         [ 0.0235,  0.0934, -0.0508, -0.0198],\n",
      "         [-0.0286,  0.1563, -0.0523,  0.0247]]], grad_fn=<CopySlices>)\n",
      "torch.Size([1, 4, 2])\n",
      "tensor([[[-0.1092,  0.0302, -0.0278, -0.1158],\n",
      "         [-0.1288,  0.3310,  0.0633,  0.0567],\n",
      "         [-0.1283,  0.4139,  0.1793,  0.1755],\n",
      "         [-0.1458,  0.4177,  0.2437,  0.2406]]], grad_fn=<CopySlices>)\n",
      "torch.Size([1, 1, 2])\n",
      "tensor([[[-0.1176,  0.1075,  0.0113, -0.0338]]], grad_fn=<CopySlices>)\n"
     ]
    }
   ],
   "source": [
    "for i in range(inputs.shape[0]):\n",
    "    curr_inputs = inputs[i, :sequence_length[i], :].unsqueeze(0)\n",
    "    outputs = torch.zeros((1, sequence_length[i], lstm.hidden_size), dtype=torch.float)\n",
    "    print(curr_inputs.shape)\n",
    "    h, c = None, None\n",
    "    for step in range(sequence_length[i]):\n",
    "        if h is None or c is None:\n",
    "            h, c = lstm(curr_inputs[:, step, :], None)\n",
    "        else:\n",
    "            h, c = lstm(curr_inputs[:, step, :], (h, c))\n",
    "        outputs[:, step, :] = h\n",
    "        \n",
    "    print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
