{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Import packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Define activation functions and their derivation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(x):\n",
    "    return (np.exp(x) - np.exp(-x)) / (np.exp(x) + np.exp(-x))\n",
    "\n",
    "def tanh_deriv(x):\n",
    "    return 1. - tanh(x) ** 2\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_deriv(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "def direct(x):\n",
    "    return x\n",
    "\n",
    "def direct_deriv(x):\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Construct a neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN:\n",
    "    def __init__(self, layers, activation_fns): # [1, 10, 1]\n",
    "        self.activations = []\n",
    "        self.activations_deriv = []\n",
    "\n",
    "        for activation_fn in activation_fns:\n",
    "\n",
    "            if activation_fn == \"tanh\":\n",
    "                self.activations.append(tanh)\n",
    "                self.activations_deriv.append(tanh_deriv)\n",
    "\n",
    "            elif activation_fn == \"sigmoid\":\n",
    "                self.activations.append(sigmoid)\n",
    "                self.activations_deriv.append(sigmoid_deriv)\n",
    "\n",
    "            elif activation_fn is None:\n",
    "                self.activations.append(direct)\n",
    "                #self.activations_deriv.append(direct)\n",
    "                self.activations_deriv.append(direct_deriv)\n",
    "        \n",
    "        self.layers = layers\n",
    "\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        \n",
    "        for l in range(len(layers) - 1):\n",
    "            self.weights.append(np.random.random([layers[l], layers[l + 1]])) # weights: [1, 10], [10, 1]\n",
    "        for l in layers:\n",
    "            self.biases.append(np.zeros(l))\n",
    "\n",
    "    def loss(self, y_pred, y):\n",
    "        return np.mean((y_pred - y) ** 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = [x + self.biases[0]]\n",
    "        for i, weight in enumerate(self.weights):\n",
    "            if self.activations[i + 1] is not None: # self.activations[0] is about input layer\n",
    "                outputs.append(self.activations[i + 1](np.matmul(outputs[i], weight) + self.biases[i + 1])) # outputs[i]: [batch_size, xx]\n",
    "            else:\n",
    "                outputs.append(np.matmul(outputs[i], weight) + self.biases[i + 1]) # outputs[i]: [batch_size, xx]\n",
    "\n",
    "        #len(outputs): 3, outputs[0...2].shape: (64, 1), (64, 30), (64, 1)\n",
    "        return outputs\n",
    "\n",
    "    def backward(self, outputs, y): # y is the label\n",
    "        batch_size = outputs[0].shape[0]\n",
    "        \n",
    "        delta_biases = [np.zeros_like(bias) for bias in self.biases]\n",
    "        delta_weights = [np.zeros_like(weight) for weight in self.weights]\n",
    "\n",
    "        for ins in range(batch_size):\n",
    "            # Calculates the error at the output layer\n",
    "            errors = []\n",
    "            errors.append(self.activations_deriv[-1](outputs[-1][ins]) * (outputs[-1][ins] - y[ins]))\n",
    "            \n",
    "            for l, weight in enumerate(reversed(self.weights)):\n",
    "                errors.append(self.activations_deriv[-(l + 2)](outputs[-(l + 2)][ins]) * np.matmul(weight, errors[l])) \n",
    "            \n",
    "            errors = list(reversed(errors))\n",
    "            \n",
    "            \n",
    "            # calc the delta\n",
    "            for i in range(len(self.layers)): # [1, 30, 1]\n",
    "                delta_biases[i] += self.learning_rate * errors[i]\n",
    "            \n",
    "            for i in range(len(self.layers) - 1):\n",
    "                output = np.array(outputs[i][ins])\n",
    "                output = output.reshape(output.shape[0], 1)\n",
    "                error = np.array(errors[i + 1]) # the first error is useless because it is the input node which has no weight\n",
    "                error = error.reshape(1, error.shape[0])\n",
    "                \n",
    "                #delta_weights[i] += self.learning_rate * np.matmul(outputs[i][ins], errors[i]) # [1, 10], O_i:[1], Err_j=Err_{j+1}:[10]\n",
    "                delta_weights[i] += self.learning_rate * np.matmul(output, error) # [1, 10], O_i:[1], Err_j=Err_{j+1}:[10]\n",
    "        \n",
    "        for i in range(1, len(self.biases)):\n",
    "            self.biases[i] -= delta_biases[i] / batch_size\n",
    "\n",
    "        for i in range(len(self.weights)):\n",
    "            self.weights[i] -= delta_weights[i] / batch_size\n",
    "\n",
    "\n",
    "\n",
    "    def fit(self, x, y, num_epochs, learning_rate=0.02):\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            outputs = self.forward(x) # len(outputs): 1\n",
    "            self.backward(outputs, y)\n",
    "\n",
    "            if epoch % 1000 == 0:\n",
    "                print(\"epoch {0}: loss={1}\".format(epoch, self.loss(outputs[-1], y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Train model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: loss=35.674792572855296\n",
      "epoch 1000: loss=9.413976089129596\n",
      "epoch 2000: loss=4.107455220412255\n",
      "epoch 3000: loss=0.29388367940048243\n",
      "epoch 4000: loss=0.17021808430170482\n",
      "epoch 5000: loss=0.13331286758264427\n",
      "epoch 6000: loss=0.11798108740107237\n",
      "epoch 7000: loss=0.11063401333019798\n",
      "epoch 8000: loss=0.1067887576064884\n",
      "epoch 9000: loss=0.10464195755352378\n",
      "array([10.94175536, 10.53181954, 10.09975448,  9.64801663,  9.17967702,\n",
      "        8.6983646 ,  8.2081739 ,  7.71354072,  7.2190933 ,  6.72948958,\n",
      "        6.24925269,  5.78261725,  5.33339767,  4.90488665,  4.49978908,\n",
      "        4.12019223,  3.76757   ,  3.44281634,  3.1463012 ,  2.87794187,\n",
      "        2.63728267,  2.423577  ,  2.23586685,  2.07305632,  1.93397694,\n",
      "        1.81744364,  1.72230135,  1.64746249,  1.59193624,  1.55485066,\n",
      "        1.53546868,  1.5331991 ,  1.54760351,  1.57839977,  1.62546281,\n",
      "        1.688823  ,  1.76866219,  1.86530746,  1.97922226,  2.11099445,\n",
      "        2.26132059,  2.4309858 ,  2.62083819,  2.8317571 ,  3.06461433,\n",
      "        3.32022792,  3.59930849,  3.90239873,  4.2298077 ,  4.58154255,\n",
      "        4.95724158,  5.35611351,  5.77688904,  6.21779067,  6.67652666,\n",
      "        7.15031373,  7.63593045,  8.12980069,  8.62810306,  9.1268988 ,\n",
      "        9.62226865, 10.11044781, 10.5879484 , 11.05166076])\n",
      "array([11.        , 10.37515747,  9.77047115,  9.18594104,  8.62156715,\n",
      "        8.07734946,  7.55328798,  7.04938272,  6.56563366,  6.10204082,\n",
      "        5.65860418,  5.23532376,  4.83219955,  4.44923154,  4.08641975,\n",
      "        3.74376417,  3.4212648 ,  3.11892164,  2.83673469,  2.57470396,\n",
      "        2.33282943,  2.11111111,  1.909549  ,  1.72814311,  1.56689342,\n",
      "        1.42579995,  1.30486269,  1.20408163,  1.12345679,  1.06298816,\n",
      "        1.02267574,  1.00251953,  1.00251953,  1.02267574,  1.06298816,\n",
      "        1.12345679,  1.20408163,  1.30486269,  1.42579995,  1.56689342,\n",
      "        1.72814311,  1.909549  ,  2.11111111,  2.33282943,  2.57470396,\n",
      "        2.83673469,  3.11892164,  3.4212648 ,  3.74376417,  4.08641975,\n",
      "        4.44923154,  4.83219955,  5.23532376,  5.65860418,  6.10204082,\n",
      "        6.56563366,  7.04938272,  7.55328798,  8.07734946,  8.62156715,\n",
      "        9.18594104,  9.77047115, 10.37515747, 11.        ])\n"
     ]
    }
   ],
   "source": [
    "def eval(model):\n",
    "    batch_size = 64\n",
    "    x = np.vstack(np.linspace(-1, 1, batch_size))\n",
    "    y = 10 * x ** 2 + 1\n",
    "    outputs = model.forward(x)\n",
    "    from pprint import pprint\n",
    "    pprint(outputs[-1].squeeze())\n",
    "    pprint(y.squeeze())\n",
    "\n",
    "# dataset\n",
    "batch_size = 64\n",
    "x = np.vstack(np.linspace(-1, 1, batch_size))\n",
    "y = 10 * x ** 2 + 1 + np.random.random([batch_size, 1])\n",
    "\n",
    "# define model\n",
    "#model = NN([1, 30, 1], [None, \"tanh\", None])\n",
    "model = NN([1, 10, 1], [None, \"tanh\", None])\n",
    "\n",
    "# train model\n",
    "model.fit(x, y, num_epochs=10000)\n",
    "\n",
    "# evaluate model\n",
    "eval(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
