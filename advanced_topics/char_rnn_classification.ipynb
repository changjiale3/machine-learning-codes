{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "char_rnn_classification.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tB8dA80iPM0Q",
        "colab_type": "text"
      },
      "source": [
        "# Char-RNN-Classification\n",
        "\n",
        "Tutorial URL: https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html\n",
        "\n",
        "## Model\n",
        "![](https://i.imgur.com/Z2xbySO.png)\n",
        "\n",
        "+ input: `(batch_size, num_letters)`, represents a character like 's'. one-hot encoding\n",
        "+ hidden: `(batch_size, hidden_size)`\n",
        "+ output: `(batch_size, num_classes)`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ram9tmnf6Dmz",
        "colab_type": "text"
      },
      "source": [
        "## 1. Import packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9NZofyA5_kX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import glob\n",
        "import random\n",
        "import string\n",
        "import unicodedata\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "If1ctXc56GSo",
        "colab_type": "text"
      },
      "source": [
        "## 2. Define constants"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tK6p6ysQ6N89",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hidden_size = 128\n",
        "num_epochs = 200000\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "giKd9Wub6IWC",
        "colab_type": "text"
      },
      "source": [
        "## 3. Prepare data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTDrbDMhBZMv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def download(url, filename=None):\n",
        "    import os\n",
        "    from urllib.request import urlretrieve\n",
        "    if filename is None:\n",
        "        filename = url.split('/')[-1]\n",
        "    if not os.path.exists(filename):\n",
        "        print('Downloading {} from {}...'.format(filename, url))\n",
        "        urlretrieve(url, filename=filename)\n",
        "\n",
        "def unzip(path):\n",
        "    from zipfile import ZipFile\n",
        "    with ZipFile(path, 'r') as z:\n",
        "        z.extractall()\n",
        "\n",
        "        download('https://download.pytorch.org/tutorial/data.zip')\n",
        "unzip('data.zip')\n",
        "\n",
        "paths = glob.glob('data/names/*.txt')\n",
        "\n",
        "categories = [path.split('/')[-1].split('.')[0] for path in paths]\n",
        "letters = string.ascii_letters + \" .,;'\"\n",
        "\n",
        "\n",
        "\n",
        "num_classes = len(categories)\n",
        "num_letters = len(letters)\n",
        "\n",
        "\n",
        "def unicode_to_ascii(s):\n",
        "    return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn' and c in letters)\n",
        "  \n",
        "\n",
        "cat2words = dict()\n",
        "for path in paths:\n",
        "    category = path.split('/')[-1].split('.')[0]\n",
        "    with open(path, 'r') as f:\n",
        "        words = f.readlines()\n",
        "        words = [unicode_to_ascii(word.strip()) for word in words]\n",
        "        cat2words[category] = words\n",
        "\n",
        "        \n",
        "\n",
        "def random_load():\n",
        "    category = random.choice(categories)\n",
        "    words = cat2words[category]\n",
        "    word = random.choice(words)\n",
        "    \n",
        "    word_tensor = torch.zeros(1, len(word), num_letters) # (1, len(word), num_letters)\n",
        "    for i in range(word_tensor.shape[1]):\n",
        "        index = letters.find(word[i])\n",
        "        word_tensor[0][i][index] = 1\n",
        "    \n",
        "    category_tensor = torch.zeros(1).long() # (1, 1)\n",
        "    index = categories.index(category)\n",
        "    category_tensor[0] = index\n",
        "    \n",
        "    return word, category, word_tensor, category_tensor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gISH1MWdC1Nn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "outputId": "55f18981-08d6-4917-f35a-cedbc2e74a0c"
      },
      "source": [
        "word, category, word_tensor, category_tensor = random_load()\n",
        "print(word)\n",
        "print(category)\n",
        "print(word_tensor.shape)\n",
        "print(word_tensor)\n",
        "print(category_tensor.shape)\n",
        "print(category_tensor)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Ngai\n",
            "Korean\n",
            "torch.Size([1, 4, 57])\n",
            "tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0.],\n",
            "         [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0.]]])\n",
            "torch.Size([1])\n",
            "tensor([10])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xV-mBvqu6J62",
        "colab_type": "text"
      },
      "source": [
        "## 4. Build model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IdtizFIgFciq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        super(Model, self).__init__()\n",
        "        self.i2h = torch.nn.Linear(input_size + hidden_size, hidden_size) # fc_hidden\n",
        "        self.i2o = torch.nn.Linear(input_size + hidden_size, num_classes) # fc_output\n",
        "        \n",
        "        self.loss_fn = torch.nn.CrossEntropyLoss()\n",
        "        self.optimizer = torch.optim.Adam(self.parameters(), lr=1e-3, betas=(0.9, 0.99))\n",
        "        \n",
        "        self.to(device)\n",
        "        \n",
        "    def _rnn(self, words, hidden):\n",
        "        inputs = torch.cat([words, hidden], 1)\n",
        "        outputs = self.i2o(inputs)\n",
        "        next_hidden = self.i2h(inputs)\n",
        "        return F.log_softmax(outputs), next_hidden\n",
        "\n",
        "    def forward(self, words): # not underlined\n",
        "        '''\n",
        "            inputs: (B, timesteps, num_letters)\n",
        "            hidden: (B, hidden_size)\n",
        "        '''\n",
        "        batch_size = words.shape[0]\n",
        "        hidden_size = self.i2h.out_features\n",
        "        hidden = torch.zeros(batch_size, hidden_size).to(device)\n",
        "        for i in range(words.shape[1]):\n",
        "            y_predicted, hidden = self._rnn(words[:, i, :], hidden)\n",
        "        \n",
        "        return y_predicted # (B, num_classes)\n",
        "\n",
        "    def fit(self, dataloader, num_epochs):\n",
        "        self.train()\n",
        "        for epoch in range(num_epochs):\n",
        "            word, category, word_tensor, category_tensor = dataloader()\n",
        "            word_tensor = word_tensor.to(device) # (len(word), 1, num_letters)\n",
        "            category_tensor = category_tensor.to(device) # (1, num_classes)\n",
        "\n",
        "            y_predicted = self.forward(word_tensor) # y_predicted.shape: (B, num_classes)\n",
        "\n",
        "            loss = self.loss_fn(y_predicted, category_tensor)\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "            \n",
        "            if epoch % 5000 == 0:\n",
        "                print('Epoch %d || loss=%.6f'%(epoch, loss.cpu().item()))\n",
        "        \n",
        "    def evaluate(self, word):\n",
        "        '''\n",
        "            word: 'Huang'.\n",
        "        '''\n",
        "        self.eval()\n",
        "        word_tensor = torch.zeros(1, len(word), num_letters) # (1, len(word) num_letters)\n",
        "        for i in range(word_tensor.shape[1]):\n",
        "            index = letters.find(word[i])\n",
        "            word_tensor[0][i][index] = 1\n",
        "        word_tensor = word_tensor.to(device)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            y_predicted = self.forward(word_tensor) # (B, num_classes)\n",
        "        probs, classes = y_predicted.topk(3)\n",
        "\n",
        "        return probs[0].cpu().numpy(), classes[0].cpu().numpy()        \n",
        "\n",
        "model = Model(num_letters, hidden_size, num_classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7Idcohd6LqN",
        "colab_type": "text"
      },
      "source": [
        "## 5. Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yry1ZYkzJDv9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 742
        },
        "outputId": "829f5168-b800-4df1-990a-0ec5fbea040e"
      },
      "source": [
        "model.fit(random_load, num_epochs=num_epochs)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:16: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  app.launch_new_instance()\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 || loss=2.879244\n",
            "Epoch 5000 || loss=0.363566\n",
            "Epoch 10000 || loss=2.020165\n",
            "Epoch 15000 || loss=3.508585\n",
            "Epoch 20000 || loss=0.792526\n",
            "Epoch 25000 || loss=0.668724\n",
            "Epoch 30000 || loss=0.374749\n",
            "Epoch 35000 || loss=0.810371\n",
            "Epoch 40000 || loss=1.935470\n",
            "Epoch 45000 || loss=0.850038\n",
            "Epoch 50000 || loss=1.800413\n",
            "Epoch 55000 || loss=0.036821\n",
            "Epoch 60000 || loss=0.752565\n",
            "Epoch 65000 || loss=1.318513\n",
            "Epoch 70000 || loss=1.424615\n",
            "Epoch 75000 || loss=0.746392\n",
            "Epoch 80000 || loss=1.523151\n",
            "Epoch 85000 || loss=1.248345\n",
            "Epoch 90000 || loss=0.193016\n",
            "Epoch 95000 || loss=0.895539\n",
            "Epoch 100000 || loss=1.498346\n",
            "Epoch 105000 || loss=0.111142\n",
            "Epoch 110000 || loss=0.665448\n",
            "Epoch 115000 || loss=0.004887\n",
            "Epoch 120000 || loss=0.565611\n",
            "Epoch 125000 || loss=0.944741\n",
            "Epoch 130000 || loss=0.004744\n",
            "Epoch 135000 || loss=0.240498\n",
            "Epoch 140000 || loss=0.018857\n",
            "Epoch 145000 || loss=0.173469\n",
            "Epoch 150000 || loss=0.079029\n",
            "Epoch 155000 || loss=2.701761\n",
            "Epoch 160000 || loss=0.167024\n",
            "Epoch 165000 || loss=3.864863\n",
            "Epoch 170000 || loss=0.586287\n",
            "Epoch 175000 || loss=3.830126\n",
            "Epoch 180000 || loss=0.400063\n",
            "Epoch 185000 || loss=0.657192\n",
            "Epoch 190000 || loss=0.353235\n",
            "Epoch 195000 || loss=3.189345\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6JzhlJ8qKute",
        "colab_type": "text"
      },
      "source": [
        "## 6. Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tBIe6UuxQhVq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "outputId": "2dc2dd03-84fc-4200-94b6-9b3cfd155f8b"
      },
      "source": [
        "name = 'Yamada'\n",
        "print(name)\n",
        "probs, classes = model.evaluate(name)\n",
        "print(probs)\n",
        "print([categories[c] for c in classes])\n",
        "\n",
        "name = 'Hinton'\n",
        "print(name)\n",
        "probs, classes = model.evaluate(name)\n",
        "print(probs)\n",
        "print([categories[c] for c in classes])\n",
        "\n",
        "name = 'Schmidhuber'\n",
        "print(name)\n",
        "probs, classes = model.evaluate(name)\n",
        "print(probs)\n",
        "print([categories[c] for c in classes])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Yamada\n",
            "[-0.03540039 -3.996563   -5.0498314 ]\n",
            "['Japanese', 'Arabic', 'Spanish']\n",
            "Hinton\n",
            "[-0.72587585 -0.997612   -3.1180897 ]\n",
            "['Scottish', 'English', 'German']\n",
            "Schmidhuber\n",
            "[-0.45987606 -1.0869541  -3.8682594 ]\n",
            "['German', 'Czech', 'English']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:16: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  app.launch_new_instance()\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}