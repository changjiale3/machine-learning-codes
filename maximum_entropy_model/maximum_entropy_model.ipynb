{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "maximum_entropy_model.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9jw0Hp6NSLo",
        "colab_type": "text"
      },
      "source": [
        "# Maximum Entropy Model\n",
        "\n",
        "## Principle of Maximum Entropy\n",
        "Entropy: $$H(P) = - \\sum_x{P(x)\\log{P(x)}}$$\n",
        "$$0 \\leq H(P) \\leq \\log{|X|}$$\n",
        "where $|X|$ is the number of possible values in $X$. If and only if the probability distribution of $X$ is uniform distribution, the information entropy would be equal to its maximum possible value, $\\log {|X|}$\n",
        "\n",
        "## Definition\n",
        "Given a training data: \n",
        "\n",
        "$$T = {(x_1, y_1), (x_2, y_2), ... , (x_N, y_N)}$$\n",
        "\n",
        "\n",
        "Empirical Probability Distribution:\n",
        "$$\\tilde{P}(x, y) = \\frac{v(x, y)}{N}  $$\n",
        "where $N$ is the size of training dataset, $v(x, y)$ is the number of times that $(x, y)$ occurs in training dataset.\n",
        "\n",
        "\n",
        "We introduce the following feature function:\n",
        "$$\n",
        "  f(x, y) = \n",
        "  \\begin{cases}\n",
        "                                   1 & \\text{if $(x, y)$ occurs in training dataset} \\\\\n",
        "                                   0 & \\text{otherwise} \\\\\n",
        "  \\end{cases}\n",
        "$$\n",
        "\n",
        "The expected value of $f(x, y)$ with respect to empirical probability distribution $\\tilde{P}(x, y)$ is equal to:\n",
        "$$\\mathbb{E}_{\\tilde{P}}(f) = \\sum_{x, y}{\\tilde{P}(x, y) f(x, y)}$$\n",
        "\n",
        "The expected value of $f(x, y)$ with respect to the model $P(y|x)$ is equal to:\n",
        "$$\\mathbb{E}_P(f) = \\sum_{x, y}{ \\tilde{P}(x) P(y|x) f(x, y) }$$\n",
        "where $\\tilde{P}(x)$ is the empirical distribution of $x$ in the training dataset and it is usually set equal to $\\frac{1}{N}$\n",
        "\n",
        "By constraining the expected value to be the equal to the empirical value, we have that:\n",
        "$$\\mathbb{E}_P(f) = \\mathbb{E}_{\\tilde{P}}(f)$$\n",
        "$$\\sum_{x, y}{ \\tilde{P}(x) P(y|x) f(x, y) } = \\sum_{x, y}{\\tilde{P}(x, y) f(x, y)} $$\n",
        "\n",
        "We have so many constrains as the number of feature functions.\n",
        "\n",
        "According to the principle of Maximum Entropy, we should select the model that is as close as possible to uniform. In other words, we should select the model $P^*$ with Maximum Entropy:\n",
        "$$P^* = \\arg\\max_{P \\in C} ( - \\sum_{x, y} \\tilde{P}(x) P(y|x) \\log P(y|x) )$$\n",
        "Given that:\n",
        "1. $P(y|x) \\geq 0$ for all $(x, y)$\n",
        "2. $\\sum_{y}{P(y|x)} = 1$ for all $x$\n",
        "3. $\\sum_{x, y}{ \\tilde{P}(x) P(y|x) f(x, y) } = \\sum_{x, y}{\\tilde{P}(x, y) f(x, y)} $ for all feature functions\n",
        "\n",
        "To solve the above optimization problem, we introduce Lagrangian multipliers. We focus on the unconstrained dual problem and we estimate the lambda free variables $\\{\\lambda_1, \\lambda_2, ... , \\lambda_n\\}$ with the Maximum Likelihood Estimation method.\n",
        "\n",
        "It can be proven that if we find the $\\{\\lambda_1, \\lambda_2, ... , \\lambda_n\\}$ parameters which maximize the dual problem, the probability given a $x$ to be classified as $y$ is equal to:\n",
        "\n",
        "$$P(y|x) = \\frac{\\exp (\\sum_{i=1}^n \\lambda_i f_i(x, y))}{  \\sum_y{    \\exp (\\sum_{i=1}^n \\lambda_i f_i(x, y)) }   }$$\n",
        "\n",
        "\n",
        "### Estimate the lambda parameters\n",
        "We use **IIS (Improved Iterative Scaling)** to estimate lambda parameters.\n",
        "\n",
        "Algorithm:\n",
        "![](http://blog.datumbox.com/wp-content/uploads/2013/11/maxentropy_tutorial/image014.png)\n",
        "\n",
        "The $f^\\#(x, y)$ is the total number of feature which are active for a particular $(x, y)$ pair. If the number is constant, then the $\\Delta{\\lambda_i}$ can be calculated in close-form:\n",
        "$$\\Delta{\\lambda_i} = \\frac{1}{M} \\log {\\frac{\\mathbb{E}_{\\tilde{P}}(f_i)}{    \\mathbb{E}_P(f_i)   }}$$\n",
        "where $M$ is equal to $f^\\#(x, y)$\n",
        "## References:\n",
        "+ [Machine Learning Tutorial: The Max Entropy Text Classifier](https://blog.datumbox.com/machine-learning-tutorial-the-max-entropy-text-classifier/)\n",
        "+ [codes](https://github.com/WenDesi/lihang_book_algorithm/blob/master/maxENT/maxENT.py)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6MtlZXu3zfI",
        "colab_type": "text"
      },
      "source": [
        "## Codes\n",
        "\n",
        "+ $P(y|x)$ is computed for 1 sample\n",
        "+ In most cases, we do not explicitly multiply $f(x, y)$ because of its definition\n",
        "+ $\\mathbb{E}_P(f)$ maps from $(x, y)$ pair to its probability. It is computed by traverse all the elements in the training data matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5DPvF27l305b",
        "colab_type": "text"
      },
      "source": [
        "### 1. Import packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o6LhbqPS4St3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import tqdm\n",
        "from collections import defaultdict\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecWdU0xX33L0",
        "colab_type": "text"
      },
      "source": [
        "### 2. Prepare data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jc5zeA_x3-Ww",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def binarization(img):\n",
        "    bin_img = img.astype(np.uint8)\n",
        "    cv2.threshold(bin_img, 50, 1, cv2.THRESH_BINARY_INV, bin_img) # pixel = 0 if value > 50 else 1\n",
        "    return bin_img\n",
        "\n",
        "raw_data = pd.read_csv('/content/gdrive/My Drive/data/train_binary.csv', header=0) # binary classification\n",
        "data = raw_data.values\n",
        "imgs = data[0:, 1:] # for one row, the first column is the label followed by the image data\n",
        "labels = data[:, 0]\n",
        "\n",
        "# reduce the size of training dataset to train faster\n",
        "# imgs = imgs[:1000] # (1000, 784)\n",
        "# labels = labels[:1000] # (1000,)\n",
        "\n",
        "# binarization\n",
        "for index, img in enumerate(imgs):\n",
        "    imgs[index] = binarization(img)\n",
        "    \n",
        "# preprocess: add subscript information to each pixel\n",
        "new_imgs = list()\n",
        "for img in imgs:\n",
        "    pixels = list()\n",
        "    for index, pixel in enumerate(img):\n",
        "        pixels.append(str(index) + '_' + str(pixel))\n",
        "    new_imgs.append(pixels)\n",
        "imgs = new_imgs # new_imgs: (1000, 784)\n",
        "# print(imgs[0]) # each element in img is a string\n",
        "    \n",
        "# choose 33% of samples for training, and the rest for testing\n",
        "x_train, x_test, y_train, y_test = train_test_split(imgs, labels, test_size=0.33, random_state=23323)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "laC6oIy_36PK",
        "colab_type": "text"
      },
      "source": [
        "### 3. Build model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KymvIUze4vNu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MaximumEntropy(object):\n",
        "    def fit(self, features, labels):\n",
        "        # store the size of training dataset\n",
        "        self.N = len(features)\n",
        "        \n",
        "        # compute the set of possible labels\n",
        "        self.labels = set(labels) \n",
        "        \n",
        "        # compute the empirical probability distribution: $\\tilde{P}(x, y)$\n",
        "        tilde_p_xy = defaultdict(int)\n",
        "        for feature, label in zip(features, labels):\n",
        "            for x in feature:\n",
        "                tilde_p_xy[(x, label)] += 1 / self.N\n",
        "            \n",
        "        # store the number of (x, y) pair in training dataset\n",
        "        self.n = len(tilde_p_xy)\n",
        "        \n",
        "        # M is equal to then number of (x, y)\n",
        "        self.M = self.n\n",
        "            \n",
        "        # define feature function: $f(x, y)$\n",
        "        self.f_xy = lambda x, y: 1 if (x, y) in tilde_p_xy else 0\n",
        "        \n",
        "        # give id to each (x, y) pair that occurs in training dataset\n",
        "        self.id2xy = list(tilde_p_xy.keys())\n",
        "        self.xy2id = {xy: id for id, xy in enumerate(self.id2xy)}\n",
        "        \n",
        "        # compute the expected value of f(x, y) with respect to empirical probability distribution: $\\mathbb{E}_{\\tilde{P}}(f)$\n",
        "        e_tilde_p_f = tilde_p_xy # since the feature function is defined as 1 if (x, y) pair appears\n",
        "        \n",
        "        # initialize lambdas with zeros\n",
        "        self.lambdas = [0.0] * self.n # each lambda corresponds to 1 (x, y) pair\n",
        "        # start iteration\n",
        "        for epoch in tqdm.tqdm(range(100)):\n",
        "            # compute the expected value of f(x, y) with respect to P(y|x)\n",
        "            e_p_f = defaultdict(float)\n",
        "            \n",
        "            for feature in features:\n",
        "                p_yx = self.get_p_yx(feature)\n",
        "                for x in feature: # traverse all the elements of training data matrix\n",
        "                    for y in self.labels:\n",
        "                        e_p_f[(x, y)] += (1 / self.N) * p_yx[y] if self.f_xy(x, y) else 0 # the feature belongs to only 1 class, so (x, y) might not exist\n",
        "                    \n",
        "            deltas = list()\n",
        "            for i in range(self.n): # focus on single lambda\n",
        "                xy = self.id2xy[i]\n",
        "                # calculate the increment of lambdas\n",
        "                delta = 1 / self.M * math.log(e_tilde_p_f[xy] / e_p_f[xy])\n",
        "                deltas.append(delta)\n",
        "            # update lambdas\n",
        "            self.lambdas = [self.lambdas[i] + deltas[i] for i in range(self.n)]\n",
        "\n",
        "    def get_p_yx(self, feature):\n",
        "        '''\n",
        "            return P(y|x) \n",
        "            \n",
        "            Note: P(y|x) is computed based on 1 sample\n",
        "        '''\n",
        "        y2probs = defaultdict(float)\n",
        "        for x in feature: # simply add all the lambdas whose (x, y) pair appears in this sample\n",
        "            for y in self.labels:\n",
        "                if (x, y) in self.xy2id.keys():\n",
        "                    y2probs[y] += self.lambdas[self.xy2id[(x, y)]]\n",
        "        for y in self.labels:\n",
        "            y2probs[y] = math.exp(y2probs[y])\n",
        "            \n",
        "        Z = sum(y2probs.values())\n",
        "        return {y: y2probs[y] / Z for y in self.labels}\n",
        "        \n",
        "    def predict(self, feature):\n",
        "        y2probs = self.get_p_yx(feature)\n",
        "        return max([(prob, y) for y, prob in y2probs.items()])[1]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fhiNj1nC372c",
        "colab_type": "text"
      },
      "source": [
        "### 4. Train and Test\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nwTbUbDINOmP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "80830505-4a20-4f96-bebe-d9d9c64035ce"
      },
      "source": [
        "me = MaximumEntropy()\n",
        "me.fit(x_train, y_train)\n",
        "def predict(features): # root.predict receive 1D array, thus we need to traverse batch\n",
        "    y_predicted = list()\n",
        "    for feature in features:\n",
        "        y_pred = me.predict(feature)\n",
        "        y_predicted.append(y_pred)\n",
        "    return np.asarray(y_predicted)\n",
        "\n",
        "y_predicted = predict(x_test)\n",
        "score = accuracy_score(y_predicted, y_test)\n",
        "print(score)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [1:54:47<00:00, 67.86s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.9660894660894661\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wvHy8GNdiF32",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}