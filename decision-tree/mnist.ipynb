{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information Gain\n",
    "Find the attribute that produces the highest information gain.\n",
    "\n",
    "+ Information Entropy:\n",
    "$Ent(D) = -\\sum_{k=1}^{|\\gamma|}{p_k \\log_2{p_k}}$, where $p_k$ represents the proportion of k-th sample in the sample set D\n",
    "\n",
    "+ Conditional Entropy:\n",
    "$Ent(Y|X) = \\sum_{i=1}^n{p_i Ent(Y|X=x_i)}$\n",
    "\n",
    "+ Information Gain:\n",
    "$Gain(D, A) = Ent(D) - Ent(D|A)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Codes\n",
    "### 1. Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/orris/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.cross_validation import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define useful functions\n",
    "+ information entropy\n",
    "+ conditional entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "information_entropy: 3.1880516698605565\n",
      "conditional_entropy: 1.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "def information_entropy(x):\n",
    "    '''\n",
    "        x: 1D array with discrete elements\n",
    "    '''\n",
    "    val2freq = dict()\n",
    "    for val in x:\n",
    "        val2freq[val] = 1 if val not in val2freq else val2freq[val] + 1\n",
    "    return sum([-freq / len(x) * np.log2(freq / len(x)) for val, freq in val2freq.items()])\n",
    "\n",
    "def conditional_entropy(y, x):\n",
    "    '''\n",
    "        Compute $Ent(Y|X) = \\sum_{i=1}^n{p_i Ent(Y|X=x_i)}$\n",
    "    \n",
    "        y: 1D array\n",
    "        x: 1D array \n",
    "    '''\n",
    "    vs = set([val for val in x])\n",
    "    res = 0.0\n",
    "    for v in vs:\n",
    "        sub_y = y[x == v] # select the subset of y according to x\n",
    "        res += len(sub_y)/ len(y) * information_entropy(sub_y)\n",
    "    return res\n",
    "\n",
    "x = np.random.randint(0, 10, size=100)\n",
    "print('information_entropy:', information_entropy(x))\n",
    "\n",
    "x = np.array([1, 0, 1, 1, 0, 1])\n",
    "y = np.array([1, 0, 3, 0, 3, 0])\n",
    "print('conditional_entropy:', conditional_entropy(y, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binarization(img):\n",
    "    bin_img = img.astype(np.uint8)\n",
    "    cv2.threshold(bin_img, 50, 1, cv2.THRESH_BINARY_INV, bin_img) # pixel = 0 if value > 50 else 1\n",
    "    return bin_img\n",
    "\n",
    "raw_data = pd.read_csv('../data/train.csv', header=0)\n",
    "data = raw_data.values\n",
    "imgs = data[0:, 1:] # for one row, the first column is the label followed by the image data\n",
    "labels = data[:, 0]\n",
    "\n",
    "# binarization\n",
    "for index, img in enumerate(imgs):\n",
    "    imgs[index] = binarization(img)\n",
    "    \n",
    "\n",
    "# 选取 2/3 数据作为训练集， 1/3 数据作为测试集\n",
    "x_train, x_test, y_train, y_test = train_test_split(imgs, labels, test_size=0.33, random_state=23323)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node(object):\n",
    "    def __init__(self, type_, label=None, attribute=None):\n",
    "        self.children = dict() # mapping from specific value to corresponding subtrees\n",
    "        self.type = type_ # leaf or internal\n",
    "        self.attribute = attribute # The attribute used for dividing data. In the case of MNIST, its value is between 0 and 784(exclusive)\n",
    "        self.label = label # The class the node is most likely to belong to\n",
    "    \n",
    "    def predict(self, feature):\n",
    "        if self.type == 'leaf':\n",
    "            return self.label\n",
    "        sub_node = self.children[feature[self.attribute]]\n",
    "        return sub_node.predict(feature)\n",
    "\n",
    "    def add_child(self, key, child):\n",
    "        self.children[key] = child\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train1(features, labels, attributes):\n",
    "    '''\n",
    "        Note: features might be a subset of training data\n",
    "    \n",
    "        features: (B, 784)\n",
    "        labels: (B,)\n",
    "    '''\n",
    "    \n",
    "        \n",
    "    # If the labels are the same, return as a leaf node\n",
    "    label_set = set(labels)\n",
    "    if len(label_set) == 1:\n",
    "        return Node('leaf', label_set.pop())\n",
    "    \n",
    "    # If there is no candidate attributes, return as a leaf node whose label is the most common label\n",
    "    label2freq = dict()\n",
    "    for label in labels:\n",
    "        label2freq[label] = 1 if label not in label2freq else label2freq[label] + 1\n",
    "    common_label, _ = max(label2freq.items(), key=lambda x:x[1])\n",
    "    if len(attributes) == 0:\n",
    "        return Node('leaf', common_label)\n",
    "\n",
    "    # Calculate the information entropy for the current set\n",
    "    ent_d = information_entropy(labels)\n",
    "    \n",
    "    # Calculate the conditional entropy for each remaining attribute\n",
    "    gains = list()\n",
    "    for attribute in attributes:\n",
    "        ent_da = conditional_entropy(labels, features[:, attribute])\n",
    "        gain_da = ent_d - ent_da\n",
    "        gains.append((gain_da, attribute))\n",
    "    # Select the attribute with the highest information gain\n",
    "    max_gain, selected_attribute = max(gains)\n",
    "    remain_attributes = attributes[:]\n",
    "    remain_attributes.remove(selected_attribute)\n",
    "    \n",
    "    node = Node('internal', attribute=selected_attribute)\n",
    "    # Select the corresponding subset from training data for each value in the chosen attribute\n",
    "#     selected_value = set([val for val in features[:, selected_attribute]])\n",
    "    selected_value = set(features[:, selected_attribute])\n",
    "\n",
    "    for val in selected_value:\n",
    "        sub_features = list()\n",
    "        sub_labels = list()\n",
    "        for feature, label in zip(features, labels):\n",
    "            if feature[selected_attribute] == val:\n",
    "                sub_features.append(feature)\n",
    "                sub_labels.append(label)\n",
    "\n",
    "        # Recursive with new subsets\n",
    "        node.add_child(val, train1(np.asarray(sub_features), np.asarray(sub_labels), remain_attributes))\n",
    "\n",
    "    return node\n",
    "\n",
    "    \n",
    "\n",
    "def train(features, labels):\n",
    "    return train1(features, labels, list(range(784)))\n",
    "\n",
    "root = train(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8603896103896104\n"
     ]
    }
   ],
   "source": [
    "def predict(features): # root.predict receive 1D array, thus we need to traverse batch\n",
    "    y_predicted = list()\n",
    "    for feature in features:\n",
    "        y_pred = root.predict(feature)\n",
    "        y_predicted.append(y_pred)\n",
    "    return np.asarray(y_predicted)\n",
    "\n",
    "y_predicted = predict(x_test)\n",
    "score = accuracy_score(y_predicted, y_test)\n",
    "print(score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
